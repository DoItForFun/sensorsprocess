2020-05-28 00:00:23.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-16d38072-5f63-4188-a548-57ce4bd139de sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:00:23.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:00:23.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-bec84896-2b2a-48a4-b60e-a549fb382fd6 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:00:23.974 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:00:23.974 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-b20901ff-1ffe-47d0-ac11-7a9db0f6e80f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:00:23.975 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:00:23.975 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:00:23.975 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:00:23.976 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:00:23.976 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:00:23.976 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:00:23.994 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:00:23.994 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:00:23.994 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:00:23.997 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:00:32.796 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 2303 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:00:32.801 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:00:33.932 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:00:33.939 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:00:33.940 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:00:33.940 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:00:34.055 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:00:34.056 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1204 ms
2020-05-28 00:00:34.299 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:00:34.600 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:00:34.639 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:00:34.640 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:00:34.640 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595234639
2020-05-28 00:00:34.641 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:00:34.643 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:00:34.646 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:00:34.652 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:00:34.652 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:00:34.652 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595234652
2020-05-28 00:00:34.652 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:00:34.652 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:00:34.654 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:00:34.659 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:00:34.659 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:00:34.659 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595234659
2020-05-28 00:00:34.659 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:00:34.659 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:00:34.660 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:00:34.681 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@46383a78 via org.mortbay.log.Slf4jLog
2020-05-28 00:00:34.694 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:00:34.710 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.418 seconds (JVM running for 3.678)
2020-05-28 00:00:34.823 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:00:34.823 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:00:34.823 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:00:34.826 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:00:34.826 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:00:34.826 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:00:34.830 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:00:34.830 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:00:34.830 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:00:34.851 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:00:34.851 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:00:34.851 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:00:34.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:00:34.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:00:34.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:00:37.861 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 83: {consumer-sensors-2-8a444e68-99b0-4650-bf35-d43072380a96=Assignment(partitions=[]), consumer-sensors-1-f161b13d-cda4-40e0-91a7-10d15b035272=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-c5c64ab1-1390-4e3f-b061-b8194e4a8d7f=Assignment(partitions=[])}
2020-05-28 00:00:37.871 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 83
2020-05-28 00:00:37.871 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 83
2020-05-28 00:00:37.871 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 83
2020-05-28 00:00:37.872 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:00:37.872 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:00:37.873 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:00:37.873 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:00:37.878 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:00:37.896 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=63, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:00:37.897 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:00:37.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:00:47.919 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:00:47.919 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:00:48.330 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:00:48.330 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:00:48.335 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:01:46.446 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:01:46.446 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-c5c64ab1-1390-4e3f-b061-b8194e4a8d7f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:01:46.446 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-8a444e68-99b0-4650-bf35-d43072380a96 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:01:46.447 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:01:46.447 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-f161b13d-cda4-40e0-91a7-10d15b035272 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:01:46.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:01:46.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:01:46.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:01:46.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:01:46.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:01:46.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:01:46.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:01:46.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:01:46.468 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:01:46.470 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:01:53.144 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 2381 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:01:53.151 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:01:54.187 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:01:54.199 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:01:54.200 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:01:54.200 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:01:54.296 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:01:54.296 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1064 ms
2020-05-28 00:01:54.524 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:01:54.812 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:01:54.851 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:01:54.851 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:01:54.852 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595314850
2020-05-28 00:01:54.853 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:01:54.854 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:01:54.857 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:01:54.863 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:01:54.864 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:01:54.864 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595314863
2020-05-28 00:01:54.864 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:01:54.864 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:01:54.865 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:01:54.870 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:01:54.871 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:01:54.871 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595314870
2020-05-28 00:01:54.871 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:01:54.871 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:01:54.872 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:01:54.892 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@61bcbcce via org.mortbay.log.Slf4jLog
2020-05-28 00:01:54.905 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:01:54.922 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.279 seconds (JVM running for 3.728)
2020-05-28 00:01:55.043 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:01:55.043 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:01:55.043 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:01:55.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:01:55.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:01:55.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:01:55.047 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:01:55.047 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:01:55.047 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:01:55.071 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:01:55.071 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:01:55.071 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:01:55.072 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:01:55.072 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:01:55.072 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:01:58.085 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 85: {consumer-sensors-3-fb157364-e44f-4d4b-91a0-f13748731743=Assignment(partitions=[]), consumer-sensors-1-5c0cdc81-5194-40e7-b346-8486c325186d=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-586420f6-e939-49ab-a3e0-c9af302fdcac=Assignment(partitions=[])}
2020-05-28 00:01:58.095 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 85
2020-05-28 00:01:58.095 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 85
2020-05-28 00:01:58.095 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 85
2020-05-28 00:01:58.095 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:01:58.095 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:01:58.095 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:01:58.095 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:01:58.097 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:01:58.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=64, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:01:58.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:01:58.143 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:02:05.455 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:02:05.455 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:02:05.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:02:05.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:02:05.887 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into table cece_data(distinct_id,event,login,properties) values(stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true,{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0});
2020-05-28 00:02:05.887 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:09:24.828 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:09:24.828 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-fb157364-e44f-4d4b-91a0-f13748731743 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:09:24.828 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-586420f6-e939-49ab-a3e0-c9af302fdcac sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:09:24.829 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:09:24.830 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-5c0cdc81-5194-40e7-b346-8486c325186d sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:09:24.831 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:09:24.831 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:09:24.831 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:09:24.831 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:09:24.831 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:09:24.831 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:09:24.848 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:09:24.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:09:24.848 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:09:24.850 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:09:31.154 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 2745 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:09:31.159 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:09:32.069 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:09:32.079 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:09:32.080 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:09:32.080 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:09:32.238 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:09:32.239 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1044 ms
2020-05-28 00:09:32.487 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:09:32.772 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:09:32.818 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:09:32.818 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:09:32.818 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595772817
2020-05-28 00:09:32.819 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:09:32.821 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:09:32.824 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:09:32.830 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:09:32.830 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:09:32.830 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595772830
2020-05-28 00:09:32.831 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:09:32.831 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:09:32.832 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:09:32.837 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:09:32.837 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:09:32.837 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595772837
2020-05-28 00:09:32.837 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:09:32.838 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:09:32.838 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:09:32.858 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@770beef5 via org.mortbay.log.Slf4jLog
2020-05-28 00:09:32.871 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:09:32.886 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.133 seconds (JVM running for 3.342)
2020-05-28 00:09:33.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:09:33.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:09:33.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:09:33.019 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:09:33.019 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:09:33.019 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:09:33.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:09:33.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:09:33.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:09:33.044 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:09:33.044 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:09:33.044 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:09:33.044 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:09:33.044 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:09:33.044 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:09:36.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 87: {consumer-sensors-2-14e38ee8-720c-4994-b8ad-678936de85ca=Assignment(partitions=[]), consumer-sensors-1-d2d804b9-8daa-4574-9e3e-5596b1459a99=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-91a4fb86-9dc6-44f2-aeda-84a3bb22ede1=Assignment(partitions=[])}
2020-05-28 00:09:36.064 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 87
2020-05-28 00:09:36.064 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 87
2020-05-28 00:09:36.064 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 87
2020-05-28 00:09:36.064 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:09:36.064 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:09:36.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:09:36.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:09:36.067 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:09:36.079 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=65, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:09:36.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:09:36.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:09:45.593 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:09:45.594 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:09:46.092 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:09:46.092 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:09:46.092 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:10:56.091 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-14e38ee8-720c-4994-b8ad-678936de85ca sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:10:56.091 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:10:56.091 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-91a4fb86-9dc6-44f2-aeda-84a3bb22ede1 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:10:56.091 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:10:56.092 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-d2d804b9-8daa-4574-9e3e-5596b1459a99 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:10:56.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:10:56.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:10:56.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:10:56.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:10:56.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:10:56.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:10:56.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:10:56.110 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:10:56.110 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:10:56.113 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:11:04.803 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 2833 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:11:04.817 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:11:06.080 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:11:06.088 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:11:06.089 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:11:06.089 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:11:06.192 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:11:06.192 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1276 ms
2020-05-28 00:11:06.536 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:11:06.823 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:11:06.865 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:11:06.865 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:11:06.865 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595866864
2020-05-28 00:11:06.867 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:11:06.868 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:11:06.872 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:11:06.879 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:11:06.879 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:11:06.879 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595866879
2020-05-28 00:11:06.879 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:11:06.879 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:11:06.880 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:11:06.885 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:11:06.885 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:11:06.885 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595866885
2020-05-28 00:11:06.885 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:11:06.886 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:11:06.886 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:11:06.908 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@3c79088e via org.mortbay.log.Slf4jLog
2020-05-28 00:11:06.923 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:11:06.940 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 3.134 seconds (JVM running for 4.589)
2020-05-28 00:11:07.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:11:07.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:11:07.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:11:07.071 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:11:07.071 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:11:07.071 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:11:07.075 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:11:07.075 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:11:07.075 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:11:07.102 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:11:07.102 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:11:07.102 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:11:07.102 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:11:07.103 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:11:07.103 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:11:10.115 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 89: {consumer-sensors-1-4cc9ffd4-daf9-4c1f-a843-e3746b035c8b=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-429bbf46-2863-4ba7-a7d7-f9105827f21b=Assignment(partitions=[]), consumer-sensors-2-48091eb7-0c1f-4678-a5f3-1fae7893bf29=Assignment(partitions=[])}
2020-05-28 00:11:10.124 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 89
2020-05-28 00:11:10.124 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 89
2020-05-28 00:11:10.124 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 89
2020-05-28 00:11:10.124 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:11:10.124 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:11:10.124 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:11:10.124 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:11:10.126 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:11:10.137 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=66, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:11:10.138 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:11:10.165 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:11:17.369 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:11:17.369 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:11:17.803 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:11:17.803 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:11:17.804 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:11:57.258 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:11:57.258 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-429bbf46-2863-4ba7-a7d7-f9105827f21b sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:11:57.258 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-48091eb7-0c1f-4678-a5f3-1fae7893bf29 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:11:57.261 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:11:57.261 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-4cc9ffd4-daf9-4c1f-a843-e3746b035c8b sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:11:57.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:11:57.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:11:57.265 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:11:57.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:11:57.265 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:11:57.265 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:11:57.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:11:57.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:11:57.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:11:57.288 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:12:04.990 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 2887 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:12:04.994 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:12:05.899 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:12:05.913 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:12:05.914 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:12:05.914 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:12:06.014 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:12:06.014 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 982 ms
2020-05-28 00:12:06.263 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:12:06.583 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:12:06.626 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:12:06.626 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:12:06.626 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595926625
2020-05-28 00:12:06.628 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:12:06.630 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:12:06.636 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:12:06.644 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:12:06.644 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:12:06.644 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595926644
2020-05-28 00:12:06.645 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:12:06.645 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:12:06.646 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:12:06.652 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:12:06.652 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:12:06.652 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590595926652
2020-05-28 00:12:06.652 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:12:06.652 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:12:06.653 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:12:06.686 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@31e76a8d via org.mortbay.log.Slf4jLog
2020-05-28 00:12:06.702 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:12:06.722 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.189 seconds (JVM running for 3.551)
2020-05-28 00:12:06.846 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:12:06.846 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:12:06.846 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:12:06.849 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:12:06.849 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:12:06.849 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:12:06.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:12:06.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:12:06.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:12:06.879 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:12:06.879 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:12:06.879 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:12:06.880 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:12:06.880 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:12:06.880 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:12:09.892 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 91: {consumer-sensors-1-5d2bc251-5537-44eb-a307-1b1edb495bf6=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-6364cdfa-ecc5-4a6e-90d1-243d773219c6=Assignment(partitions=[]), consumer-sensors-3-95c01a9d-afc9-4fb5-8238-9457080291fe=Assignment(partitions=[])}
2020-05-28 00:12:09.900 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 91
2020-05-28 00:12:09.900 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 91
2020-05-28 00:12:09.900 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 91
2020-05-28 00:12:09.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:12:09.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:12:09.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:12:09.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:12:09.905 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:12:09.918 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=67, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:12:09.919 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:12:09.951 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:12:16.623 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:12:16.623 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:12:17.131 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:12:17.132 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:12:17.137 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Seeking to offset 67 for partition cece_sensors-0
2020-05-28 00:12:17.138 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:157) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:1867) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1773) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:1701) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:1599) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1330) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1062) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:1879) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 10 more
Caused by: java.lang.NullPointerException
	at com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl.insert(SensorsHiveDaoImpl.java:26) ~[classes/:?]
	at com.queue.process.kafka.service.impl.SensorsServiceImpl.send(SensorsServiceImpl.java:51) ~[classes/:?]
	at com.queue.process.kafka.controller.SensorsController.process(SensorsController.java:23) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:48) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:334) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:86) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:51) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:1834) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:1817) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1760) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 8 more
2020-05-28 00:12:17.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:12:17.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:12:17.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:12:17.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:12:17.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Seeking to offset 67 for partition cece_sensors-0
2020-05-28 00:12:17.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:157) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:1867) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1773) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:1701) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:1599) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1330) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1062) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:1879) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 10 more
Caused by: java.lang.NullPointerException
	at com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl.insert(SensorsHiveDaoImpl.java:26) ~[classes/:?]
	at com.queue.process.kafka.service.impl.SensorsServiceImpl.send(SensorsServiceImpl.java:51) ~[classes/:?]
	at com.queue.process.kafka.controller.SensorsController.process(SensorsController.java:23) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:48) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:334) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:86) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:51) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:1834) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:1817) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1760) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 8 more
2020-05-28 00:12:17.667 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:12:17.667 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:12:17.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:12:17.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:12:17.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Seeking to offset 67 for partition cece_sensors-0
2020-05-28 00:12:17.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:157) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:1867) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1773) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:1701) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:1599) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1330) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1062) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:1879) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 10 more
Caused by: java.lang.NullPointerException
	at com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl.insert(SensorsHiveDaoImpl.java:26) ~[classes/:?]
	at com.queue.process.kafka.service.impl.SensorsServiceImpl.send(SensorsServiceImpl.java:51) ~[classes/:?]
	at com.queue.process.kafka.controller.SensorsController.process(SensorsController.java:23) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:48) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:334) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:86) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:51) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:1834) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:1817) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1760) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 8 more
2020-05-28 00:12:18.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:12:18.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:12:18.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:12:18.220 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:12:18.220 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Seeking to offset 67 for partition cece_sensors-0
2020-05-28 00:12:18.220 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:157) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:1867) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1773) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:1701) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:1599) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1330) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1062) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:1879) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 10 more
Caused by: java.lang.NullPointerException
	at com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl.insert(SensorsHiveDaoImpl.java:26) ~[classes/:?]
	at com.queue.process.kafka.service.impl.SensorsServiceImpl.send(SensorsServiceImpl.java:51) ~[classes/:?]
	at com.queue.process.kafka.controller.SensorsController.process(SensorsController.java:23) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:48) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:334) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:86) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:51) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:1834) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:1817) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1760) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 8 more
2020-05-28 00:12:18.691 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:12:18.691 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:12:18.732 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:12:18.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:12:18.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Seeking to offset 67 for partition cece_sensors-0
2020-05-28 00:12:18.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:157) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:1867) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1773) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:1701) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:1599) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1330) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1062) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:1879) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 10 more
Caused by: java.lang.NullPointerException
	at com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl.insert(SensorsHiveDaoImpl.java:26) ~[classes/:?]
	at com.queue.process.kafka.service.impl.SensorsServiceImpl.send(SensorsServiceImpl.java:51) ~[classes/:?]
	at com.queue.process.kafka.controller.SensorsController.process(SensorsController.java:23) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:48) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:334) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:86) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:51) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:1834) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:1817) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1760) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 8 more
2020-05-28 00:12:19.203 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:12:19.203 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:12:19.244 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:12:19.244 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:12:19.245 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Seeking to offset 67 for partition cece_sensors-0
2020-05-28 00:12:19.245 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:157) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:1867) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1773) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:1701) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:1599) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1330) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1062) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:1879) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 10 more
Caused by: java.lang.NullPointerException
	at com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl.insert(SensorsHiveDaoImpl.java:26) ~[classes/:?]
	at com.queue.process.kafka.service.impl.SensorsServiceImpl.send(SensorsServiceImpl.java:51) ~[classes/:?]
	at com.queue.process.kafka.controller.SensorsController.process(SensorsController.java:23) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:48) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:334) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:86) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:51) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:1834) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:1817) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1760) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 8 more
2020-05-28 00:12:19.720 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:12:19.720 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:12:19.760 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:12:19.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:12:19.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Seeking to offset 67 for partition cece_sensors-0
2020-05-28 00:12:19.762 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:157) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:1867) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1773) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:1701) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:1599) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1330) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1062) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:1879) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 10 more
Caused by: java.lang.NullPointerException
	at com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl.insert(SensorsHiveDaoImpl.java:26) ~[classes/:?]
	at com.queue.process.kafka.service.impl.SensorsServiceImpl.send(SensorsServiceImpl.java:51) ~[classes/:?]
	at com.queue.process.kafka.controller.SensorsController.process(SensorsController.java:23) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:48) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:334) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:86) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:51) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:1834) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:1817) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1760) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 8 more
2020-05-28 00:12:20.234 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:12:20.234 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:12:20.279 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:12:20.279 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:12:20.279 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Seeking to offset 67 for partition cece_sensors-0
2020-05-28 00:12:20.280 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:157) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:103) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:1867) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1773) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:1701) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:1599) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1330) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1062) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.queue.process.kafka.controller.SensorsController.process(java.lang.String)' threw exception; nested exception is java.lang.NullPointerException; nested exception is java.lang.NullPointerException
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:1879) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 10 more
Caused by: java.lang.NullPointerException
	at com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl.insert(SensorsHiveDaoImpl.java:26) ~[classes/:?]
	at com.queue.process.kafka.service.impl.SensorsServiceImpl.send(SensorsServiceImpl.java:51) ~[classes/:?]
	at com.queue.process.kafka.controller.SensorsController.process(SensorsController.java:23) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) ~[spring-messaging-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:48) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:334) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:86) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:51) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:1834) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:1817) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:1760) ~[spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	... 8 more
2020-05-28 00:12:20.534 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-6364cdfa-ecc5-4a6e-90d1-243d773219c6 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:12:20.534 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:12:20.534 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-95c01a9d-afc9-4fb5-8238-9457080291fe sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:12:20.535 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:12:20.535 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-5d2bc251-5537-44eb-a307-1b1edb495bf6 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:12:20.536 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:12:20.536 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:12:20.536 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:12:20.536 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:12:20.536 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:12:20.536 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:12:20.569 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:12:20.569 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:12:20.569 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:12:20.572 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:13:50.264 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 2982 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:13:50.272 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:13:51.145 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:13:51.151 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:13:51.152 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:13:51.152 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:13:51.240 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:13:51.240 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 917 ms
2020-05-28 00:13:51.482 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:13:51.768 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:13:51.809 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:13:51.809 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:13:51.809 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596031808
2020-05-28 00:13:51.811 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:13:51.812 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:13:51.815 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:13:51.821 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:13:51.821 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:13:51.821 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596031821
2020-05-28 00:13:51.821 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:13:51.821 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:13:51.822 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:13:51.827 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:13:51.827 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:13:51.827 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596031827
2020-05-28 00:13:51.827 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:13:51.828 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:13:51.828 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:13:51.848 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@7cdbaa50 via org.mortbay.log.Slf4jLog
2020-05-28 00:13:51.862 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:13:51.878 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.069 seconds (JVM running for 3.118)
2020-05-28 00:13:51.993 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:13:51.993 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:13:51.993 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:13:51.995 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:13:51.995 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:13:51.995 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:13:51.997 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:13:51.997 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:13:51.997 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:13:52.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:13:52.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:13:52.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:13:52.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:13:52.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:13:52.021 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:13:55.033 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 93: {consumer-sensors-2-7cfc2b05-154e-4362-8212-02740fa1df10=Assignment(partitions=[]), consumer-sensors-3-1398d819-dd84-4918-8d9f-8e9f979a5456=Assignment(partitions=[]), consumer-sensors-1-25c37900-b365-48c5-96ef-6c10708a91b9=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 00:13:55.042 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 93
2020-05-28 00:13:55.042 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 93
2020-05-28 00:13:55.042 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 93
2020-05-28 00:13:55.042 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:13:55.042 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:13:55.043 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:13:55.043 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:13:55.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:13:55.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=67, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:13:55.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:13:55.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:13:55.150 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:13:55.150 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:13:55.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:13:55.580 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:13:55.580 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:14:31.915 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-1398d819-dd84-4918-8d9f-8e9f979a5456 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:14:31.915 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:14:31.915 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-7cfc2b05-154e-4362-8212-02740fa1df10 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:14:31.915 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:14:31.915 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-25c37900-b365-48c5-96ef-6c10708a91b9 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:14:31.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:14:31.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:14:31.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:14:31.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:14:31.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:14:31.917 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:14:31.935 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:14:31.937 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:14:31.937 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:14:31.939 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:14:38.098 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 3031 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:14:38.102 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:14:38.984 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:14:38.991 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:14:38.991 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:14:38.992 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:14:39.073 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:14:39.074 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 941 ms
2020-05-28 00:14:39.305 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:14:39.596 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:14:39.639 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:14:39.639 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:14:39.640 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596079638
2020-05-28 00:14:39.642 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:14:39.643 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:14:39.647 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:14:39.655 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:14:39.655 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:14:39.655 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596079655
2020-05-28 00:14:39.655 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:14:39.655 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:14:39.657 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:14:39.663 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:14:39.663 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:14:39.663 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596079663
2020-05-28 00:14:39.664 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:14:39.664 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:14:39.665 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:14:39.687 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@310aee0b via org.mortbay.log.Slf4jLog
2020-05-28 00:14:39.702 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:14:39.719 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.022 seconds (JVM running for 3.362)
2020-05-28 00:14:39.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:14:39.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:14:39.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:14:39.850 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:14:39.850 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:14:39.850 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:14:39.853 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:14:39.853 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:14:39.853 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:14:39.881 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:14:39.881 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:14:39.881 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:14:39.882 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:14:39.882 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:14:39.882 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:14:42.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 95: {consumer-sensors-1-f697cab8-bede-4cb6-a4c5-e0d187f9a961=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-456d09af-bf66-4f14-afc0-6e350fc2a5dd=Assignment(partitions=[]), consumer-sensors-3-0b10f332-892b-404d-b39d-64cfbc898984=Assignment(partitions=[])}
2020-05-28 00:14:42.921 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 95
2020-05-28 00:14:42.921 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 95
2020-05-28 00:14:42.921 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 95
2020-05-28 00:14:42.922 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:14:42.922 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:14:42.923 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:14:42.923 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:14:42.926 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:14:42.940 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=68, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:14:42.940 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:14:42.969 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:14:53.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:14:53.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:14:53.718 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:14:53.718 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:14:53.719 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:15:10.203 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-456d09af-bf66-4f14-afc0-6e350fc2a5dd sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:15:10.203 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:15:10.203 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-0b10f332-892b-404d-b39d-64cfbc898984 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:15:10.207 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:15:10.207 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-f697cab8-bede-4cb6-a4c5-e0d187f9a961 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:15:10.209 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:15:10.209 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:15:10.209 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:15:10.210 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:15:10.210 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:15:10.210 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:15:10.277 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:15:10.277 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:15:10.277 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:15:10.284 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:15:16.693 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 3069 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:15:16.699 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:15:17.682 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:15:17.690 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:15:17.690 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:15:17.690 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:15:17.796 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:15:17.796 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1048 ms
2020-05-28 00:15:18.045 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:15:18.346 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:15:18.395 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:15:18.395 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:15:18.395 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596118394
2020-05-28 00:15:18.397 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:15:18.398 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:15:18.402 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:15:18.408 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:15:18.408 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:15:18.408 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596118408
2020-05-28 00:15:18.408 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:15:18.409 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:15:18.410 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:15:18.416 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:15:18.416 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:15:18.416 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596118416
2020-05-28 00:15:18.417 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:15:18.417 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:15:18.418 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:15:18.449 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@1d247525 via org.mortbay.log.Slf4jLog
2020-05-28 00:15:18.462 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:15:18.480 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.369 seconds (JVM running for 3.446)
2020-05-28 00:15:18.624 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:15:18.624 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:15:18.624 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:15:18.626 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:15:18.626 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:15:18.626 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:15:18.630 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:15:18.630 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:15:18.630 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:15:18.658 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:15:18.658 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:15:18.658 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:15:18.658 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:15:18.658 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:15:18.658 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:15:21.671 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 97: {consumer-sensors-2-f9ee0c74-b6f6-43dc-8db0-0a2025cb3525=Assignment(partitions=[]), consumer-sensors-3-1937ab6c-fbcf-4fb5-8d68-d4fad7dd483b=Assignment(partitions=[]), consumer-sensors-1-95b3d065-25e3-4aa0-a6ca-2a4965614e77=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 00:15:21.681 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 97
2020-05-28 00:15:21.681 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 97
2020-05-28 00:15:21.681 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 97
2020-05-28 00:15:21.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:15:21.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:15:21.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:15:21.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:15:21.683 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:15:21.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=69, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:15:21.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:15:21.721 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:15:29.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:15:29.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:15:29.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:15:29.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:15:29.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:21:15.002 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Offset commit failed on partition cece_sensors-0 at offset 70: The request timed out.
2020-05-28 00:21:15.004 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:21:15.063 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:23:40.433 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-1937ab6c-fbcf-4fb5-8d68-d4fad7dd483b sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:23:40.433 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:23:40.433 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-f9ee0c74-b6f6-43dc-8db0-0a2025cb3525 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:23:40.435 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:23:40.436 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-95b3d065-25e3-4aa0-a6ca-2a4965614e77 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:23:40.438 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:23:40.437 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:23:40.437 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:23:40.438 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:23:40.438 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:23:40.438 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:23:40.459 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:23:40.459 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:23:40.460 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:23:40.463 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:23:47.626 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 3462 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:23:47.630 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:23:48.666 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:23:48.674 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:23:48.674 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:23:48.674 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:23:48.786 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:23:48.786 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1111 ms
2020-05-28 00:23:49.046 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:23:49.374 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:23:49.418 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:23:49.418 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:23:49.418 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596629417
2020-05-28 00:23:49.420 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:23:49.421 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:23:49.425 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:23:49.431 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:23:49.431 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:23:49.431 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596629431
2020-05-28 00:23:49.432 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:23:49.432 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:23:49.433 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:23:49.438 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:23:49.439 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:23:49.439 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596629438
2020-05-28 00:23:49.439 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:23:49.439 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:23:49.440 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:23:49.462 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@414f13fc via org.mortbay.log.Slf4jLog
2020-05-28 00:23:49.482 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:23:49.502 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.578 seconds (JVM running for 3.982)
2020-05-28 00:23:49.647 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:23:49.647 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:23:49.647 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:23:49.651 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:23:49.651 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:23:49.651 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:23:49.655 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:23:49.655 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:23:49.655 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:23:49.684 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:23:49.684 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:23:49.685 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:23:49.684 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:23:49.685 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:23:49.685 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:23:52.697 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 99: {consumer-sensors-1-77445c25-6599-4ec4-bdde-6f01976a2bc4=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-06892eb3-f4d8-40ab-a388-bd3070d97005=Assignment(partitions=[]), consumer-sensors-3-7d840e4f-d43c-448d-b7b2-1dbe2dd8a522=Assignment(partitions=[])}
2020-05-28 00:23:52.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 99
2020-05-28 00:23:52.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 99
2020-05-28 00:23:52.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 99
2020-05-28 00:23:52.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:23:52.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:23:52.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:23:52.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:23:52.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:23:52.721 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=70, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:23:52.722 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:23:52.751 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:23:59.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:23:59.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:23:59.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:23:59.917 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:23:59.917 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:24:26.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-06892eb3-f4d8-40ab-a388-bd3070d97005 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:24:26.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-7d840e4f-d43c-448d-b7b2-1dbe2dd8a522 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:24:26.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:24:26.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:24:26.603 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-77445c25-6599-4ec4-bdde-6f01976a2bc4 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:24:26.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:24:26.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:24:26.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:24:26.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:24:26.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:24:26.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:24:26.617 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:24:26.618 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:24:26.618 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:24:26.620 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:24:36.645 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 3514 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:24:36.650 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:24:38.667 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:24:38.677 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:24:38.677 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:24:38.678 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:24:38.817 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:24:38.817 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 2131 ms
2020-05-28 00:24:39.107 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:24:39.456 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:24:39.504 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:24:39.504 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:24:39.504 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596679503
2020-05-28 00:24:39.506 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:24:39.507 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:24:39.510 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:24:39.516 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:24:39.516 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:24:39.516 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596679516
2020-05-28 00:24:39.516 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:24:39.517 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:24:39.518 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:24:39.525 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:24:39.526 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:24:39.526 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596679525
2020-05-28 00:24:39.527 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:24:39.527 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:24:39.529 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:24:39.551 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@f324455 via org.mortbay.log.Slf4jLog
2020-05-28 00:24:39.565 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:24:39.583 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 3.956 seconds (JVM running for 5.513)
2020-05-28 00:24:39.713 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:24:39.713 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:24:39.713 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:24:39.716 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:24:39.716 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:24:39.716 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:24:39.720 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:24:39.720 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:24:39.720 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:24:39.744 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:24:39.744 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:24:39.744 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:24:39.745 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:24:39.745 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:24:39.745 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:24:42.760 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 101: {consumer-sensors-1-9eda6d95-d507-41d0-a147-8dccb22f4c76=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-04d2ba90-0957-443b-80e5-b18a4e65bd31=Assignment(partitions=[]), consumer-sensors-2-7f35575f-89ee-4455-8be4-9a5691cd0f4c=Assignment(partitions=[])}
2020-05-28 00:24:42.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 101
2020-05-28 00:24:42.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 101
2020-05-28 00:24:42.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 101
2020-05-28 00:24:42.772 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:24:42.772 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:24:42.773 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:24:42.773 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:24:42.775 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:24:42.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=71, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:24:42.788 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:24:42.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:24:50.140 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:24:50.141 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:24:50.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:24:50.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:24:50.674 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:25:08.796 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Offset commit failed on partition cece_sensors-0 at offset 72: The request timed out.
2020-05-28 00:25:08.797 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:25:08.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:25:46.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-7f35575f-89ee-4455-8be4-9a5691cd0f4c sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:25:46.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-04d2ba90-0957-443b-80e5-b18a4e65bd31 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:25:46.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:25:46.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:25:46.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-9eda6d95-d507-41d0-a147-8dccb22f4c76 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:25:46.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:25:46.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:25:46.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:25:46.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:25:46.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:25:46.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:25:46.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:25:46.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:25:46.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:25:46.198 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:25:52.571 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 3575 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:25:52.575 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:25:53.607 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:25:53.616 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:25:53.617 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:25:53.617 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:25:53.853 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:25:53.853 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1233 ms
2020-05-28 00:25:54.099 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:25:54.392 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:25:54.434 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:25:54.434 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:25:54.434 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596754433
2020-05-28 00:25:54.435 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:25:54.437 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:25:54.440 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:25:54.446 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:25:54.446 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:25:54.446 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596754446
2020-05-28 00:25:54.447 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:25:54.447 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:25:54.449 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:25:54.456 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:25:54.456 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:25:54.457 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596754456
2020-05-28 00:25:54.457 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:25:54.457 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:25:54.458 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:25:54.479 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@46383a78 via org.mortbay.log.Slf4jLog
2020-05-28 00:25:54.497 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:25:54.513 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.471 seconds (JVM running for 3.914)
2020-05-28 00:25:54.634 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:25:54.634 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:25:54.634 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:25:54.639 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:25:54.639 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:25:54.639 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:25:54.643 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:25:54.643 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:25:54.643 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:25:54.674 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:25:54.674 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:25:54.674 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:25:54.674 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:25:54.674 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:25:54.674 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:25:57.686 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 103: {consumer-sensors-2-d64fbed7-4118-4b6f-855d-88c74400b769=Assignment(partitions=[]), consumer-sensors-1-e397a603-803b-45a3-bc67-cede88828b98=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-c52d90fe-fa98-4c14-9140-360362345d32=Assignment(partitions=[])}
2020-05-28 00:25:57.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 103
2020-05-28 00:25:57.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 103
2020-05-28 00:25:57.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 103
2020-05-28 00:25:57.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:25:57.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:25:57.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:25:57.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:25:57.698 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:25:57.710 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=72, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:25:57.710 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:25:57.741 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:26:05.749 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:26:05.749 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:26:06.167 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:26:06.167 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:26:06.167 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:27:18.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-c52d90fe-fa98-4c14-9140-360362345d32 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:27:18.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:27:18.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-d64fbed7-4118-4b6f-855d-88c74400b769 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:27:18.403 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:27:18.408 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:27:18.408 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:27:18.408 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:27:18.408 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:27:18.409 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-e397a603-803b-45a3-bc67-cede88828b98 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:27:18.410 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:27:18.411 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:27:18.413 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:27:18.416 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:27:18.418 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:27:18.420 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:27:24.602 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 3652 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:27:24.608 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:27:25.542 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:27:25.553 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:27:25.554 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:27:25.555 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:27:25.755 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:27:25.756 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1107 ms
2020-05-28 00:27:26.018 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:27:26.332 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:27:26.373 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:27:26.373 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:27:26.373 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596846372
2020-05-28 00:27:26.374 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:27:26.376 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:27:26.379 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:27:26.385 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:27:26.385 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:27:26.386 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596846385
2020-05-28 00:27:26.386 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:27:26.386 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:27:26.387 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:27:26.392 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:27:26.392 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:27:26.392 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596846392
2020-05-28 00:27:26.392 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:27:26.392 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:27:26.393 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:27:26.413 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@e76b097 via org.mortbay.log.Slf4jLog
2020-05-28 00:27:26.427 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:27:26.443 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.319 seconds (JVM running for 3.43)
2020-05-28 00:27:26.565 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:27:26.565 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:27:26.565 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:27:26.568 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:27:26.568 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:27:26.568 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:27:26.571 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:27:26.571 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:27:26.571 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:27:26.596 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:27:26.596 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:27:26.596 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:27:26.597 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:27:26.597 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:27:26.597 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:27:29.610 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 105: {consumer-sensors-2-464f94f8-ce04-4cad-9756-ca2b12be3554=Assignment(partitions=[]), consumer-sensors-1-0ac7bfa2-dc53-48ee-98a6-650a77de06b8=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-dc1907c2-7244-4b67-b445-1ddc8bef2863=Assignment(partitions=[])}
2020-05-28 00:27:29.623 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 105
2020-05-28 00:27:29.623 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 105
2020-05-28 00:27:29.623 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 105
2020-05-28 00:27:29.623 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:27:29.623 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:27:29.624 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:27:29.624 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:27:29.630 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:27:29.651 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=73, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:27:29.652 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:27:29.686 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:27:34.719 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:27:34.719 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:27:35.145 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:27:35.146 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:27:35.146 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:28:00.248 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:28:00.248 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-dc1907c2-7244-4b67-b445-1ddc8bef2863 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:28:00.248 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-464f94f8-ce04-4cad-9756-ca2b12be3554 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:28:00.248 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:28:00.249 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-0ac7bfa2-dc53-48ee-98a6-650a77de06b8 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:28:00.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:28:00.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:28:00.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:28:00.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:28:00.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:28:00.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:28:00.265 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:28:00.266 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:28:00.266 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:28:00.268 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:28:07.443 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 3688 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:28:07.448 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:28:08.543 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:28:08.552 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:28:08.552 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:28:08.552 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:28:08.662 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:28:08.663 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1172 ms
2020-05-28 00:28:09.102 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:28:09.442 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:28:09.495 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:28:09.495 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:28:09.495 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596889494
2020-05-28 00:28:09.496 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:28:09.498 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:28:09.502 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:28:09.510 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:28:09.510 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:28:09.510 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596889510
2020-05-28 00:28:09.511 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:28:09.511 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:28:09.512 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:28:09.518 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:28:09.519 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:28:09.519 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596889518
2020-05-28 00:28:09.519 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:28:09.519 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:28:09.520 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:28:09.552 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@4fea840f via org.mortbay.log.Slf4jLog
2020-05-28 00:28:09.568 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:28:09.590 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.619 seconds (JVM running for 5.35)
2020-05-28 00:28:09.749 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:28:09.749 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:28:09.749 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:28:09.752 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:28:09.752 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:28:09.752 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:28:09.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:28:09.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:28:09.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:28:09.785 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:28:09.785 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:28:09.785 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:28:09.785 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:28:09.785 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:28:09.785 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:28:12.811 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 107: {consumer-sensors-2-5b0f3ac8-1bbb-4dd6-a76e-44653a4e2419=Assignment(partitions=[]), consumer-sensors-1-447e105e-faad-4dfd-ba63-51f1b2660e4c=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-393e4012-8022-4b2a-b334-ef4b0dd6e431=Assignment(partitions=[])}
2020-05-28 00:28:12.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 107
2020-05-28 00:28:12.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 107
2020-05-28 00:28:12.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 107
2020-05-28 00:28:12.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:28:12.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:28:12.820 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:28:12.820 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:28:12.824 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:28:12.835 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=74, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:28:12.836 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:28:12.861 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:28:19.531 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:28:19.532 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:28:19.992 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:28:19.993 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:28:19.995 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:28:37.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:28:37.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-5b0f3ac8-1bbb-4dd6-a76e-44653a4e2419 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:28:37.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-393e4012-8022-4b2a-b334-ef4b0dd6e431 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:28:37.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:28:37.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-447e105e-faad-4dfd-ba63-51f1b2660e4c sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:28:37.184 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:28:37.184 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:28:37.184 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:28:37.184 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:28:37.184 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:28:37.184 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:28:37.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:28:37.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:28:37.214 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:28:37.221 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:28:43.929 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 3720 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:28:43.934 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:28:45.116 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:28:45.128 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:28:45.129 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:28:45.129 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:28:45.328 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:28:45.329 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1353 ms
2020-05-28 00:28:45.721 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:28:46.079 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:28:46.131 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:28:46.131 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:28:46.131 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596926130
2020-05-28 00:28:46.132 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:28:46.134 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:28:46.138 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:28:46.146 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:28:46.146 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:28:46.147 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596926146
2020-05-28 00:28:46.147 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:28:46.147 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:28:46.149 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:28:46.156 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:28:46.156 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:28:46.157 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590596926156
2020-05-28 00:28:46.157 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:28:46.157 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:28:46.158 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:28:46.189 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@1ee40b5c via org.mortbay.log.Slf4jLog
2020-05-28 00:28:46.208 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:28:46.232 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.82 seconds (JVM running for 4.724)
2020-05-28 00:28:46.494 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:28:46.494 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:28:46.494 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:28:46.495 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:28:46.495 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:28:46.495 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:28:46.498 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:28:46.498 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:28:46.498 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:28:46.525 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:28:46.525 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:28:46.525 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:28:46.526 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:28:46.526 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:28:46.526 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:28:49.547 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 109: {consumer-sensors-3-66294a64-8f08-49c4-bb75-759d2ce52c02=Assignment(partitions=[]), consumer-sensors-2-b20dc90b-3ec5-4f6d-9d91-909128150c3c=Assignment(partitions=[]), consumer-sensors-1-e748a24b-adc7-4bbd-bfe5-70854fa90b04=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 00:28:49.557 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 109
2020-05-28 00:28:49.557 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 109
2020-05-28 00:28:49.557 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 109
2020-05-28 00:28:49.558 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:28:49.558 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:28:49.560 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:28:49.560 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:28:49.565 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:28:49.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=75, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:28:49.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:28:49.604 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:28:54.671 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:28:54.671 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:28:55.506 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:28:55.506 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:29:19.515 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.174 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 00:35:35.175 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.195 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.195 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.201 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.206 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.207 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.207 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.207 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.213 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.213 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.225 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.225 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.231 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.231 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.231 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.231 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.237 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.237 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.237 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:35:35.243 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:35:35.255 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Attempt to heartbeat failed for since member id consumer-sensors-3-66294a64-8f08-49c4-bb75-759d2ce52c02 is not valid.
2020-05-28 00:35:35.256 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:35:35.262 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Attempt to heartbeat failed for since member id consumer-sensors-2-b20dc90b-3ec5-4f6d-9d91-909128150c3c is not valid.
2020-05-28 00:35:35.262 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:35:35.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:35:35.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:35:35.272 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:35:35.272 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:35:35.301 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-186a1c5e-cad5-4d83-b647-b292c8a39491 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:35:35.301 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-f9e9e8c0-e882-4bd2-ae0b-d86e585351f9 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:35:35.301 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:35:35.301 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:35:35.302 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:35:35.302 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:35:35.304 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:35:35.304 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:35:35.304 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:35:35.304 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:35:35.308 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:35:38.277 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:35:38.279 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:35:38.281 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:35:46.739 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4040 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:35:46.743 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:35:47.713 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:35:47.720 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:35:47.721 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:35:47.721 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:35:47.816 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:35:47.816 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1036 ms
2020-05-28 00:35:47.875 [main] WARN  org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sensorsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'org.apache.tomcat.jdbc.pool.DataSource' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=), @org.springframework.beans.factory.annotation.Qualifier(value=hiveDruidDataSource)}
2020-05-28 00:35:47.877 [main] INFO  org.apache.catalina.core.StandardService - Stopping service [Tomcat]
2020-05-28 00:35:48.070 [main] ERROR org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

A component required a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' that could not be found.


Action:

Consider defining a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' in your configuration.

2020-05-28 00:35:48.072 [main] WARN  org.springframework.boot.SpringApplication - Unable to close ApplicationContext
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.retrieveApplicationListeners(AbstractApplicationEventMulticaster.java:245) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.getApplicationListeners(AbstractApplicationEventMulticaster.java:197) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:134) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:403) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:360) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:81) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:67) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:167) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:978) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.SpringApplication.handleRunFailure(SpringApplication.java:814) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:325) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at com.queue.process.QueueApplication.main(QueueApplication.java:10) [classes/:?]
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:814) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1282) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:297) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.annotation.ConfigurationClassPostProcessor$ImportAwareBeanPostProcessor.postProcessBeforeInitialization(ConfigurationClassPostProcessor.java:456) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:416) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1788) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
2020-05-28 00:36:55.366 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4101 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:36:55.372 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:36:56.259 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:36:56.266 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:36:56.267 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:36:56.267 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:36:56.369 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:36:56.369 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 953 ms
2020-05-28 00:36:56.426 [main] WARN  org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sensorsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'org.apache.tomcat.jdbc.pool.DataSource' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=), @org.springframework.beans.factory.annotation.Qualifier(value=hiveDruidDataSource)}
2020-05-28 00:36:56.428 [main] INFO  org.apache.catalina.core.StandardService - Stopping service [Tomcat]
2020-05-28 00:36:56.618 [main] ERROR org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

A component required a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' that could not be found.


Action:

Consider defining a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' in your configuration.

2020-05-28 00:36:56.621 [main] WARN  org.springframework.boot.SpringApplication - Unable to close ApplicationContext
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.retrieveApplicationListeners(AbstractApplicationEventMulticaster.java:245) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.getApplicationListeners(AbstractApplicationEventMulticaster.java:197) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:134) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:403) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:360) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:81) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:67) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:167) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:978) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.SpringApplication.handleRunFailure(SpringApplication.java:814) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:325) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at com.queue.process.QueueApplication.main(QueueApplication.java:10) [classes/:?]
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:814) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1282) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:297) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.annotation.ConfigurationClassPostProcessor$ImportAwareBeanPostProcessor.postProcessBeforeInitialization(ConfigurationClassPostProcessor.java:456) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:416) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1788) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
2020-05-28 00:40:16.080 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4247 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:40:16.084 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:40:17.056 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:40:17.062 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:40:17.063 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:40:17.063 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:40:17.145 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:40:17.145 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1018 ms
2020-05-28 00:40:17.207 [main] WARN  org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sensorsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'org.apache.tomcat.jdbc.pool.DataSource' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=), @org.springframework.beans.factory.annotation.Qualifier(value=hiveDruidDataSource)}
2020-05-28 00:40:17.210 [main] INFO  org.apache.catalina.core.StandardService - Stopping service [Tomcat]
2020-05-28 00:40:17.390 [main] ERROR org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

A component required a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' that could not be found.


Action:

Consider defining a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' in your configuration.

2020-05-28 00:40:17.391 [main] WARN  org.springframework.boot.SpringApplication - Unable to close ApplicationContext
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.retrieveApplicationListeners(AbstractApplicationEventMulticaster.java:245) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.getApplicationListeners(AbstractApplicationEventMulticaster.java:197) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:134) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:403) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:360) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:81) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:67) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:167) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:978) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.SpringApplication.handleRunFailure(SpringApplication.java:814) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:325) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at com.queue.process.QueueApplication.main(QueueApplication.java:10) [classes/:?]
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:814) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1282) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:297) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.annotation.ConfigurationClassPostProcessor$ImportAwareBeanPostProcessor.postProcessBeforeInitialization(ConfigurationClassPostProcessor.java:456) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:416) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1788) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
2020-05-28 00:42:48.235 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4365 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:42:48.244 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:42:49.237 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:42:49.247 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:42:49.247 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:42:49.248 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:42:49.342 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:42:49.342 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1055 ms
2020-05-28 00:42:49.399 [main] WARN  org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sensorsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'org.apache.tomcat.jdbc.pool.DataSource' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=), @org.springframework.beans.factory.annotation.Qualifier(value=hiveDruidDataSource)}
2020-05-28 00:42:49.402 [main] INFO  org.apache.catalina.core.StandardService - Stopping service [Tomcat]
2020-05-28 00:42:49.577 [main] ERROR org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

A component required a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' that could not be found.


Action:

Consider defining a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' in your configuration.

2020-05-28 00:42:49.581 [main] WARN  org.springframework.boot.SpringApplication - Unable to close ApplicationContext
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.retrieveApplicationListeners(AbstractApplicationEventMulticaster.java:245) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.getApplicationListeners(AbstractApplicationEventMulticaster.java:197) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:134) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:403) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:360) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:81) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:67) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:167) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:978) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.SpringApplication.handleRunFailure(SpringApplication.java:814) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:325) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at com.queue.process.QueueApplication.main(QueueApplication.java:10) [classes/:?]
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:814) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1282) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:297) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.annotation.ConfigurationClassPostProcessor$ImportAwareBeanPostProcessor.postProcessBeforeInitialization(ConfigurationClassPostProcessor.java:456) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:416) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1788) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
2020-05-28 00:43:11.515 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4387 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:43:11.520 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:43:12.617 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:43:12.625 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:43:12.625 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:43:12.626 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:43:12.726 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:43:12.726 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1164 ms
2020-05-28 00:43:12.948 [main] WARN  org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sensorsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'org.apache.tomcat.jdbc.pool.DataSource' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=), @org.springframework.beans.factory.annotation.Qualifier(value=hiveDruidDataSource)}
2020-05-28 00:43:12.954 [main] INFO  org.apache.catalina.core.StandardService - Stopping service [Tomcat]
2020-05-28 00:43:13.087 [main] ERROR org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

A component required a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' that could not be found.


Action:

Consider defining a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' in your configuration.

2020-05-28 00:43:13.090 [main] WARN  org.springframework.boot.SpringApplication - Unable to close ApplicationContext
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.retrieveApplicationListeners(AbstractApplicationEventMulticaster.java:245) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.getApplicationListeners(AbstractApplicationEventMulticaster.java:197) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:134) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:403) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:360) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:81) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:67) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:167) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:978) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.SpringApplication.handleRunFailure(SpringApplication.java:814) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:325) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at com.queue.process.QueueApplication.main(QueueApplication.java:10) [classes/:?]
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:814) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1282) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:297) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.annotation.ConfigurationClassPostProcessor$ImportAwareBeanPostProcessor.postProcessBeforeInitialization(ConfigurationClassPostProcessor.java:456) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:416) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1788) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
2020-05-28 00:43:28.880 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4403 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:43:28.885 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:43:30.527 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:43:30.536 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:43:30.536 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:43:30.536 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:43:30.681 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:43:30.682 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1758 ms
2020-05-28 00:43:31.031 [main] WARN  org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sensorsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'org.apache.tomcat.jdbc.pool.DataSource' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=), @org.springframework.beans.factory.annotation.Qualifier(value=hiveDruidDataSource)}
2020-05-28 00:43:31.038 [main] INFO  org.apache.catalina.core.StandardService - Stopping service [Tomcat]
2020-05-28 00:43:31.208 [main] ERROR org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter - 

***************************
APPLICATION FAILED TO START
***************************

Description:

A component required a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' that could not be found.


Action:

Consider defining a bean of type 'org.apache.tomcat.jdbc.pool.DataSource' in your configuration.

2020-05-28 00:43:31.211 [main] WARN  org.springframework.boot.SpringApplication - Unable to close ApplicationContext
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.retrieveApplicationListeners(AbstractApplicationEventMulticaster.java:245) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.AbstractApplicationEventMulticaster.getApplicationListeners(AbstractApplicationEventMulticaster.java:197) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:134) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:403) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:360) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:81) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.availability.AvailabilityChangeEvent.publish(AvailabilityChangeEvent.java:67) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:167) ~[spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:978) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.SpringApplication.handleRunFailure(SpringApplication.java:814) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:325) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at com.queue.process.QueueApplication.main(QueueApplication.java:10) [classes/:?]
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration': Initialization of bean failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:603) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry' available
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:814) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1282) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:297) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.annotation.ConfigurationClassPostProcessor$ImportAwareBeanPostProcessor.postProcessBeforeInitialization(ConfigurationClassPostProcessor.java:456) ~[spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:416) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1788) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:409) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans(BeanFactoryAdvisorRetrievalHelper.java:91) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findCandidateAdvisors(AbstractAdvisorAutoProxyCreator.java:109) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.findEligibleAdvisors(AbstractAdvisorAutoProxyCreator.java:94) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean(AbstractAdvisorAutoProxyCreator.java:76) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:347) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:299) ~[spring-aop-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:431) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	... 28 more
2020-05-28 00:45:28.381 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4499 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:45:28.388 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:45:29.341 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:45:29.348 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:45:29.349 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:45:29.349 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:45:29.436 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:45:29.437 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1006 ms
2020-05-28 00:45:29.674 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:45:29.964 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:45:30.013 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:45:30.013 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:45:30.013 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590597930012
2020-05-28 00:45:30.015 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:45:30.016 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:45:30.020 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:45:30.026 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:45:30.026 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:45:30.026 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590597930026
2020-05-28 00:45:30.027 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:45:30.027 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:45:30.028 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:45:30.034 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:45:30.034 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:45:30.034 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590597930034
2020-05-28 00:45:30.034 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:45:30.034 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:45:30.035 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:45:30.057 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@12a14b74 via org.mortbay.log.Slf4jLog
2020-05-28 00:45:30.072 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:45:30.089 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.197 seconds (JVM running for 3.702)
2020-05-28 00:45:30.210 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:45:30.210 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:45:30.210 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:45:30.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:45:30.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:45:30.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:45:30.216 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:45:30.216 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:45:30.216 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:45:30.241 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:45:30.241 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:45:30.241 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:45:30.242 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:45:30.242 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:45:30.242 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:45:33.255 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 113: {consumer-sensors-3-ff48d269-1a40-42d3-bb61-7a99751e56c1=Assignment(partitions=[]), consumer-sensors-2-0bee078c-0511-401e-b83e-871514d83eed=Assignment(partitions=[]), consumer-sensors-1-1d0f0bf2-89f0-4200-bd05-af097931afd3=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 00:45:33.266 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 113
2020-05-28 00:45:33.266 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 113
2020-05-28 00:45:33.266 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 113
2020-05-28 00:45:33.266 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:45:33.266 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:45:33.267 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:45:33.267 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:45:33.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:45:33.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=75, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:45:33.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:45:33.310 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:45:33.383 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:45:33.383 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:45:33.814 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:45:33.815 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:45:33.842 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 00:45:33.843 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 00:45:34.947 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 00:45:34.969 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: ParseException line 1:120 cannot recognize input near 'stff000001c0c480d43abf611d3dwenWenConsultingExperttrue' '{' '"problem_description"' in expression specification
2020-05-28 00:45:50.283 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Offset commit failed on partition cece_sensors-0 at offset 76: The request timed out.
2020-05-28 00:45:50.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 00:45:50.681 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:47:47.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-ff48d269-1a40-42d3-bb61-7a99751e56c1 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:47:47.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:47:47.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-0bee078c-0511-401e-b83e-871514d83eed sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:47:47.770 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:47:47.770 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-1d0f0bf2-89f0-4200-bd05-af097931afd3 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:47:47.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:47:47.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:47:47.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:47:47.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:47:47.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:47:47.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:47:47.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:47:47.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:47:47.788 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:47:47.790 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:47:47.791 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 00:48:18.754 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4638 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:48:18.760 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:48:19.593 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:48:19.603 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:48:19.603 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:48:19.604 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:48:19.716 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:48:19.717 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 924 ms
2020-05-28 00:48:19.960 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:48:20.330 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:48:20.373 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:48:20.373 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:48:20.373 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598100372
2020-05-28 00:48:20.375 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:48:20.376 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:48:20.379 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:48:20.386 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:48:20.386 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:48:20.386 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598100386
2020-05-28 00:48:20.386 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:48:20.387 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:48:20.388 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:48:20.393 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:48:20.393 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:48:20.393 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598100393
2020-05-28 00:48:20.393 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:48:20.394 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:48:20.395 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:48:20.414 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@7e7fe6d via org.mortbay.log.Slf4jLog
2020-05-28 00:48:20.430 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:48:20.447 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.448 seconds (JVM running for 4.454)
2020-05-28 00:48:20.577 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:48:20.577 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:48:20.577 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:48:20.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:48:20.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:48:20.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:48:20.582 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:48:20.582 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:48:20.582 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:48:20.608 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:48:20.608 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:48:20.608 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:48:20.609 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:48:20.609 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:48:20.609 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:48:23.621 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 115: {consumer-sensors-3-f215e365-e2f3-4abd-bc08-df864c52ef31=Assignment(partitions=[]), consumer-sensors-1-f49403b0-b999-4c94-8f72-d37b29b844f3=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-e1179416-7404-474f-a2c1-c81f89612120=Assignment(partitions=[])}
2020-05-28 00:48:23.630 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 115
2020-05-28 00:48:23.630 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 115
2020-05-28 00:48:23.630 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 115
2020-05-28 00:48:23.631 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:48:23.631 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:48:23.631 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:48:23.631 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:48:23.634 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:48:23.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=76, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:48:23.649 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:48:23.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:48:33.385 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:48:33.386 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:48:33.863 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:48:33.864 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:48:33.876 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 00:48:33.877 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 00:48:34.871 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 00:48:34.891 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: ParseException line 1:123 cannot recognize input near '{' '"problem_description"' ':' in expression specification
2020-05-28 00:49:17.259 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-f215e365-e2f3-4abd-bc08-df864c52ef31 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:49:17.259 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:49:17.259 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-e1179416-7404-474f-a2c1-c81f89612120 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:49:17.261 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:49:17.261 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-f49403b0-b999-4c94-8f72-d37b29b844f3 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:49:17.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:49:17.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:49:17.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:49:17.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:49:17.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:49:17.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:49:17.280 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:49:17.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:49:17.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:49:17.283 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:49:17.284 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 00:49:23.402 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4694 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:49:23.408 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:49:24.301 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:49:24.312 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:49:24.313 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:49:24.313 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:49:24.422 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:49:24.422 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 959 ms
2020-05-28 00:49:24.669 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:49:24.952 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:49:24.991 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:49:24.992 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:49:24.992 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598164991
2020-05-28 00:49:24.993 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:49:24.994 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:49:24.998 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:49:25.004 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:49:25.004 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:49:25.005 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598165004
2020-05-28 00:49:25.005 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:49:25.005 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:49:25.007 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:49:25.012 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:49:25.012 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:49:25.012 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598165012
2020-05-28 00:49:25.012 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:49:25.012 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:49:25.013 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:49:25.039 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@17092fff via org.mortbay.log.Slf4jLog
2020-05-28 00:49:25.053 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:49:25.070 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.218 seconds (JVM running for 3.21)
2020-05-28 00:49:25.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:49:25.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:49:25.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:49:25.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:49:25.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:49:25.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:49:25.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:49:25.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:49:25.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:49:25.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:49:25.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:49:25.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:49:25.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:49:25.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:49:25.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:49:28.216 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 117: {consumer-sensors-1-55a0d367-777e-480d-930c-f6ef021e6a94=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-cb277eb1-4952-489c-a866-8f3406a4b34e=Assignment(partitions=[]), consumer-sensors-3-ae8fd6cd-94c6-4785-b636-194ed6350b03=Assignment(partitions=[])}
2020-05-28 00:49:28.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 117
2020-05-28 00:49:28.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 117
2020-05-28 00:49:28.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 117
2020-05-28 00:49:28.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:49:28.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:49:28.226 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:49:28.226 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:49:28.228 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:49:28.240 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=77, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:49:28.240 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:49:28.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:49:33.355 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:49:33.355 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:49:33.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:49:33.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:49:33.778 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 00:49:33.778 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 00:49:34.762 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 00:49:34.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data(distinct_id,event,login,properties) values( stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true,{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0});
2020-05-28 00:49:34.780 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: ParseException line 1:123 cannot recognize input near '{' '"problem_description"' ':' in expression specification
2020-05-28 00:50:56.029 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:50:56.029 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-ae8fd6cd-94c6-4785-b636-194ed6350b03 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:50:56.029 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-cb277eb1-4952-489c-a866-8f3406a4b34e sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:50:56.031 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:50:56.031 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-55a0d367-777e-480d-930c-f6ef021e6a94 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:50:56.034 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:50:56.034 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:50:56.034 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:50:56.035 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:50:56.035 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:50:56.035 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:50:56.051 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:50:56.052 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:50:56.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:50:56.054 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:50:56.055 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 00:51:02.404 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4778 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:51:02.408 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:51:03.278 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:51:03.285 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:51:03.286 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:51:03.286 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:51:03.384 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:51:03.384 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 938 ms
2020-05-28 00:51:03.694 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:51:03.995 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:51:04.045 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:51:04.045 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:51:04.046 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598264044
2020-05-28 00:51:04.047 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:51:04.048 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:51:04.052 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:51:04.058 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:51:04.058 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:51:04.059 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598264058
2020-05-28 00:51:04.059 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:51:04.059 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:51:04.060 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:51:04.066 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:51:04.066 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:51:04.066 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598264066
2020-05-28 00:51:04.066 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:51:04.067 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:51:04.068 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:51:04.089 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@5922d3e9 via org.mortbay.log.Slf4jLog
2020-05-28 00:51:04.104 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:51:04.121 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.143 seconds (JVM running for 3.429)
2020-05-28 00:51:04.238 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:51:04.238 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:51:04.238 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:51:04.239 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:51:04.239 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:51:04.239 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:51:04.242 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:51:04.242 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:51:04.242 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:51:04.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:51:04.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:51:04.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:51:04.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:51:04.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:51:04.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:51:07.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 119: {consumer-sensors-2-eacd3e81-9782-4413-b4cf-7e6c31a5c642=Assignment(partitions=[]), consumer-sensors-1-fe866b61-c2fc-4115-b238-4bb4a68e3eb1=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-060d9842-d9dd-4bb1-9c06-ce6a897d3984=Assignment(partitions=[])}
2020-05-28 00:51:07.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 119
2020-05-28 00:51:07.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 119
2020-05-28 00:51:07.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 119
2020-05-28 00:51:07.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:51:07.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:51:07.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:51:07.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:51:07.293 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:51:07.305 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=78, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:51:07.306 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:51:07.336 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:51:14.359 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:51:14.360 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:51:14.783 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:51:14.783 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:51:14.797 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 00:51:14.798 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 00:51:15.794 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 00:51:15.799 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data(distinct_id,event,login,properties) values( stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true,{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0});
2020-05-28 00:51:15.814 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: ParseException line 1:123 cannot recognize input near '{' '"problem_description"' ':' in expression specification
2020-05-28 00:53:32.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-060d9842-d9dd-4bb1-9c06-ce6a897d3984 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:53:32.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:53:32.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-eacd3e81-9782-4413-b4cf-7e6c31a5c642 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:53:32.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:53:32.912 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-fe866b61-c2fc-4115-b238-4bb4a68e3eb1 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:53:32.913 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:53:32.913 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:53:32.913 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:53:32.913 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:53:32.913 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:53:32.913 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:53:32.925 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:53:32.929 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:53:32.930 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:53:32.932 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:53:32.934 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 00:53:40.244 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 4971 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:53:40.250 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:53:41.245 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:53:41.255 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:53:41.255 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:53:41.256 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:53:41.353 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:53:41.354 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1066 ms
2020-05-28 00:53:41.606 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:53:41.902 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:53:41.944 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:53:41.944 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:53:41.944 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598421943
2020-05-28 00:53:41.945 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:53:41.947 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:53:41.951 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:53:41.960 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:53:41.961 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:53:41.961 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598421960
2020-05-28 00:53:41.961 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:53:41.961 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:53:41.963 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:53:41.970 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:53:41.970 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:53:41.970 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598421970
2020-05-28 00:53:41.970 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:53:41.970 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:53:41.971 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:53:41.991 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@36327cec via org.mortbay.log.Slf4jLog
2020-05-28 00:53:42.006 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:53:42.027 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.262 seconds (JVM running for 3.577)
2020-05-28 00:53:42.148 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:53:42.148 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:53:42.148 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:53:42.150 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:53:42.150 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:53:42.150 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:53:42.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:53:42.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:53:42.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:53:42.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:53:42.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:53:42.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:53:42.183 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:53:42.183 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:53:42.183 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:53:45.196 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 121: {consumer-sensors-2-04953949-7def-48e6-9329-fcdd43f0db6a=Assignment(partitions=[]), consumer-sensors-3-a00e9b82-ba1c-4731-a8bb-513c62d932a4=Assignment(partitions=[]), consumer-sensors-1-fb988f4f-7879-4039-84d9-d105a9a694bb=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 00:53:45.208 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 121
2020-05-28 00:53:45.208 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 121
2020-05-28 00:53:45.208 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 121
2020-05-28 00:53:45.208 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:53:45.209 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:53:45.209 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:53:45.209 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:53:45.212 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:53:45.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=79, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:53:45.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:53:45.255 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:53:52.160 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:53:52.161 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:53:52.547 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:53:52.547 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:53:52.559 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 00:53:52.559 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 00:53:53.600 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 00:53:53.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data(distinct_id,event,login,properties) values( stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true,);
2020-05-28 00:53:53.615 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: ParseException line 1:123 cannot recognize input near ')' ';' '<EOF>' in expression specification
2020-05-28 00:56:21.983 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-a00e9b82-ba1c-4731-a8bb-513c62d932a4 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:56:21.983 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:56:21.983 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-04953949-7def-48e6-9329-fcdd43f0db6a sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:56:21.987 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:56:21.987 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-fb988f4f-7879-4039-84d9-d105a9a694bb sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:56:21.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:56:21.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:56:21.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:56:21.990 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:56:21.990 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:56:21.990 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:56:22.004 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:56:22.004 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:56:22.005 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:56:22.007 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:56:22.008 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 00:56:27.969 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5256 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:56:27.974 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:56:28.842 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:56:28.849 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:56:28.850 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:56:28.850 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:56:28.957 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:56:28.957 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 937 ms
2020-05-28 00:56:29.193 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:56:29.483 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:56:29.522 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:56:29.522 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:56:29.523 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598589521
2020-05-28 00:56:29.524 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:56:29.525 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:56:29.528 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:56:29.535 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:56:29.535 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:56:29.535 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598589535
2020-05-28 00:56:29.535 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:56:29.535 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:56:29.537 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:56:29.542 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:56:29.542 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:56:29.542 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598589542
2020-05-28 00:56:29.542 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:56:29.543 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:56:29.544 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:56:29.564 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@541179e7 via org.mortbay.log.Slf4jLog
2020-05-28 00:56:29.577 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:56:29.593 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.075 seconds (JVM running for 3.019)
2020-05-28 00:56:29.697 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:56:29.697 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:56:29.697 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:56:29.699 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:56:29.699 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:56:29.699 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:56:29.701 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:56:29.701 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:56:29.701 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:56:29.726 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:56:29.726 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:56:29.726 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:56:29.726 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:56:29.726 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:56:29.727 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:56:32.738 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 123: {consumer-sensors-1-f33a2378-d12f-4fe9-b14f-6cac2e178cb0=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-e1d6296e-cfa2-42f0-9f04-a19b36fa7927=Assignment(partitions=[]), consumer-sensors-2-bd002653-2395-46e5-8ae6-e04a7dfad758=Assignment(partitions=[])}
2020-05-28 00:56:32.747 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 123
2020-05-28 00:56:32.747 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 123
2020-05-28 00:56:32.747 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 123
2020-05-28 00:56:32.748 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:56:32.748 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:56:32.748 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:56:32.748 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:56:32.750 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:56:32.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=80, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:56:32.762 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:56:32.788 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:56:43.133 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:56:43.133 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:56:43.594 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:56:43.594 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:56:43.610 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 00:56:43.610 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 00:56:44.603 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 00:56:44.604 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data(distinct_id,event,login,properties) values( stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true, '' );
2020-05-28 00:56:44.620 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: ParseException line 1:128 extraneous input ';' expecting EOF near '<EOF>'
2020-05-28 00:57:06.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-e1d6296e-cfa2-42f0-9f04-a19b36fa7927 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:57:06.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:57:06.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-bd002653-2395-46e5-8ae6-e04a7dfad758 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:57:06.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:57:06.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-f33a2378-d12f-4fe9-b14f-6cac2e178cb0 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:57:06.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:57:06.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:57:06.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:57:06.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:57:06.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:57:06.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:57:06.223 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:57:06.224 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:57:06.224 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:57:06.227 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:57:06.230 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 00:57:12.173 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5301 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:57:12.177 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:57:12.993 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:57:13.000 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:57:13.000 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:57:13.000 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:57:13.108 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:57:13.108 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 892 ms
2020-05-28 00:57:13.362 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:57:13.661 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:57:13.704 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:57:13.705 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:57:13.705 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598633704
2020-05-28 00:57:13.706 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:57:13.707 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:57:13.711 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:57:13.717 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:57:13.717 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:57:13.717 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598633717
2020-05-28 00:57:13.717 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:57:13.718 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:57:13.719 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:57:13.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:57:13.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:57:13.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598633724
2020-05-28 00:57:13.724 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:57:13.724 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:57:13.725 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:57:13.746 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@4fb392c4 via org.mortbay.log.Slf4jLog
2020-05-28 00:57:13.759 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:57:13.777 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.075 seconds (JVM running for 3.128)
2020-05-28 00:57:13.892 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:57:13.892 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:57:13.892 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:57:13.894 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:57:13.894 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:57:13.894 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:57:13.897 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:57:13.897 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:57:13.897 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:57:13.922 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:57:13.922 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:57:13.922 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:57:13.923 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:57:13.923 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:57:13.923 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:57:16.935 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 125: {consumer-sensors-2-8414be85-a414-4682-8276-c5935c76c74d=Assignment(partitions=[]), consumer-sensors-3-bf380424-f9e4-4650-b5de-45918344c03f=Assignment(partitions=[]), consumer-sensors-1-cbb6d4dd-4b89-4f0e-854c-b571e927fe0a=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 00:57:16.943 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 125
2020-05-28 00:57:16.943 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 125
2020-05-28 00:57:16.943 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 125
2020-05-28 00:57:16.943 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:57:16.943 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:57:16.944 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:57:16.944 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:57:16.946 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:57:16.959 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=81, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:57:16.960 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:57:16.988 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:57:23.222 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:57:23.222 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:57:23.665 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:57:23.666 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:57:23.684 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 00:57:23.685 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 00:57:24.671 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 00:57:24.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data(distinct_id,event,login,properties) values( stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true, '' )
2020-05-28 00:57:24.938 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_TABLE_OR_COL not supported in insert/values
2020-05-28 00:59:27.555 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 00:59:27.555 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-8414be85-a414-4682-8276-c5935c76c74d sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:59:27.555 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-bf380424-f9e4-4650-b5de-45918344c03f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:59:27.557 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:59:27.557 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-cbb6d4dd-4b89-4f0e-854c-b571e927fe0a sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 00:59:27.558 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:59:27.558 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:59:27.558 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 00:59:27.559 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:59:27.559 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:59:27.559 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 00:59:27.569 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:59:27.570 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:59:27.572 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 00:59:27.575 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 00:59:27.579 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 00:59:34.128 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5432 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 00:59:34.132 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 00:59:35.017 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 00:59:35.030 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 00:59:35.030 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 00:59:35.030 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 00:59:35.116 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 00:59:35.116 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 954 ms
2020-05-28 00:59:35.365 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 00:59:35.649 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:59:35.691 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:59:35.691 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:59:35.691 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598775690
2020-05-28 00:59:35.693 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:59:35.695 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:59:35.699 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:59:35.705 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:59:35.705 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:59:35.705 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598775705
2020-05-28 00:59:35.706 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:59:35.706 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:59:35.707 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 00:59:35.716 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 00:59:35.716 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 00:59:35.716 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598775716
2020-05-28 00:59:35.717 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 00:59:35.717 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 00:59:35.718 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 00:59:35.741 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@54d1608f via org.mortbay.log.Slf4jLog
2020-05-28 00:59:35.754 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 00:59:35.772 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.072 seconds (JVM running for 3.46)
2020-05-28 00:59:35.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:59:35.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:59:35.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 00:59:35.903 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:59:35.903 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:59:35.903 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 00:59:35.906 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:59:35.906 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:59:35.906 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:59:35.931 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:59:35.931 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:59:35.931 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 00:59:35.932 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 00:59:35.932 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 00:59:35.932 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 00:59:38.944 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 127: {consumer-sensors-3-b4e66769-7561-4699-90e0-27e53be89a2a=Assignment(partitions=[]), consumer-sensors-1-c103a307-3be0-4477-b3df-0c18d327de0f=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-a0b1add3-9ca0-4ce8-9b12-429f26fc7ac3=Assignment(partitions=[])}
2020-05-28 00:59:38.954 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 127
2020-05-28 00:59:38.954 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 127
2020-05-28 00:59:38.954 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 127
2020-05-28 00:59:38.954 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:59:38.954 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 00:59:38.955 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:59:38.955 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 00:59:38.956 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 00:59:38.969 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=82, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 00:59:38.969 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 00:59:38.996 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 00:59:54.709 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 00:59:54.709 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 00:59:55.199 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 00:59:55.199 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 00:59:55.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 00:59:55.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 00:59:56.330 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 00:59:56.331 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into selectcece_data(distinct_id,event,login,properties) values( stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true, '' )
2020-05-28 00:59:56.551 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found selectcece_data
2020-05-28 01:00:05.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:00:05.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-b4e66769-7561-4699-90e0-27e53be89a2a sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:00:05.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-a0b1add3-9ca0-4ce8-9b12-429f26fc7ac3 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:00:05.063 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:00:05.063 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-c103a307-3be0-4477-b3df-0c18d327de0f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:00:05.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:00:05.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:00:05.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:00:05.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:00:05.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:00:05.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:00:05.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:00:05.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:00:05.081 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:00:05.083 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:00:05.084 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:00:11.725 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5468 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:00:11.733 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:00:12.786 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:00:12.793 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:00:12.793 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:00:12.794 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:00:12.893 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:00:12.893 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1114 ms
2020-05-28 01:00:13.134 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:00:13.459 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:00:13.501 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:00:13.502 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:00:13.502 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598813501
2020-05-28 01:00:13.503 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:00:13.504 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:00:13.508 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:00:13.515 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:00:13.515 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:00:13.515 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598813515
2020-05-28 01:00:13.515 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:00:13.516 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:00:13.517 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:00:13.522 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:00:13.522 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:00:13.522 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598813522
2020-05-28 01:00:13.523 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:00:13.523 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:00:13.523 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:00:13.544 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@737db7f8 via org.mortbay.log.Slf4jLog
2020-05-28 01:00:13.558 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:00:13.576 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.47 seconds (JVM running for 3.707)
2020-05-28 01:00:13.700 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:00:13.700 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:00:13.700 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:00:13.703 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:00:13.703 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:00:13.703 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:00:13.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:00:13.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:00:13.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:00:13.732 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:00:13.732 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:00:13.732 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:00:13.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:00:13.734 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:00:13.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:00:16.746 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 129: {consumer-sensors-3-2ee72574-5c58-4fd1-bc5f-3692da6f67cf=Assignment(partitions=[]), consumer-sensors-1-3f432aa5-bfdd-4408-a09f-d081e4504ab9=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-dca3bf6e-a538-47ad-9a50-7f4822f20500=Assignment(partitions=[])}
2020-05-28 01:00:16.755 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 129
2020-05-28 01:00:16.755 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 129
2020-05-28 01:00:16.755 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 129
2020-05-28 01:00:16.755 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:00:16.755 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:00:16.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:00:16.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:00:16.757 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:00:16.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=83, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:00:16.769 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:00:16.795 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:00:45.574 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:00:45.574 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-2ee72574-5c58-4fd1-bc5f-3692da6f67cf sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:00:45.574 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-dca3bf6e-a538-47ad-9a50-7f4822f20500 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:00:45.576 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:00:45.576 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-3f432aa5-bfdd-4408-a09f-d081e4504ab9 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:00:45.578 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:00:45.578 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:00:45.578 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:00:45.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:00:45.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:00:45.579 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:00:45.599 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:00:45.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:00:45.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:00:45.607 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:00:51.903 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5504 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:00:51.907 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:00:52.771 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:00:52.778 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:00:52.779 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:00:52.779 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:00:52.886 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:00:52.886 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 943 ms
2020-05-28 01:00:53.128 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:00:53.423 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:00:53.464 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:00:53.464 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:00:53.464 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598853463
2020-05-28 01:00:53.466 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:00:53.467 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:00:53.470 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:00:53.476 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:00:53.476 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:00:53.476 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598853476
2020-05-28 01:00:53.477 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:00:53.477 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:00:53.478 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:00:53.483 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:00:53.483 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:00:53.483 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598853483
2020-05-28 01:00:53.483 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:00:53.484 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:00:53.484 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:00:53.505 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@559cedee via org.mortbay.log.Slf4jLog
2020-05-28 01:00:53.518 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:00:53.534 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.128 seconds (JVM running for 3.207)
2020-05-28 01:00:53.643 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:00:53.643 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:00:53.643 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:00:53.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:00:53.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:00:53.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:00:53.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:00:53.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:00:53.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:00:53.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:00:53.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:00:53.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:00:53.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:00:53.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:00:53.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:00:56.685 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 131: {consumer-sensors-2-e225c619-7c19-4cc8-ba7a-7e6985fa645d=Assignment(partitions=[]), consumer-sensors-3-eb678f50-6b60-4ed9-a796-3a8a442392a2=Assignment(partitions=[]), consumer-sensors-1-118cb230-bab6-425d-a333-0fdcebf5d693=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:00:56.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 131
2020-05-28 01:00:56.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 131
2020-05-28 01:00:56.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 131
2020-05-28 01:00:56.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:00:56.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:00:56.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:00:56.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:00:56.697 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:00:56.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=83, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:00:56.709 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:00:56.738 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:01:03.757 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:01:03.757 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:01:04.142 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:01:04.142 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:01:04.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:01:04.155 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:01:05.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:01:05.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data select stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true, '' )
2020-05-28 01:01:05.186 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: ParseException line 1:90 extraneous input ')' expecting EOF near '<EOF>'
2020-05-28 01:01:35.907 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:01:35.907 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-e225c619-7c19-4cc8-ba7a-7e6985fa645d sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:01:35.907 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-eb678f50-6b60-4ed9-a796-3a8a442392a2 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:01:35.908 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:01:35.909 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-118cb230-bab6-425d-a333-0fdcebf5d693 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:01:35.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:01:35.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:01:35.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:01:35.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:01:35.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:01:35.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:01:35.925 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:01:35.926 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:01:35.926 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:01:35.928 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:01:35.929 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:01:42.157 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5551 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:01:42.162 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:01:42.976 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:01:42.985 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:01:42.985 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:01:42.986 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:01:43.113 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:01:43.114 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 920 ms
2020-05-28 01:01:43.343 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:01:43.625 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:01:43.667 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:01:43.667 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:01:43.667 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598903666
2020-05-28 01:01:43.669 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:01:43.670 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:01:43.674 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:01:43.680 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:01:43.681 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:01:43.681 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598903680
2020-05-28 01:01:43.681 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:01:43.681 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:01:43.682 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:01:43.687 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:01:43.687 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:01:43.687 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598903687
2020-05-28 01:01:43.687 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:01:43.688 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:01:43.688 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:01:43.709 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@11eed657 via org.mortbay.log.Slf4jLog
2020-05-28 01:01:43.722 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:01:43.741 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.0 seconds (JVM running for 3.28)
2020-05-28 01:01:43.853 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:01:43.853 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:01:43.853 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:01:43.855 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:01:43.855 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:01:43.855 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:01:43.858 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:01:43.858 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:01:43.858 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:01:43.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:01:43.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:01:43.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:01:43.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:01:43.884 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:01:43.884 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:01:46.899 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 133: {consumer-sensors-1-2a080205-5af9-4ea1-a885-86723d37d561=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-f7c1f4ef-9bed-4036-b53c-5984edcf3e5e=Assignment(partitions=[]), consumer-sensors-3-81943252-60cb-4f4b-8218-7b69deb550ba=Assignment(partitions=[])}
2020-05-28 01:01:46.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 133
2020-05-28 01:01:46.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 133
2020-05-28 01:01:46.910 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 133
2020-05-28 01:01:46.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:01:46.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:01:46.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:01:46.911 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:01:46.913 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:01:46.926 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=84, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:01:46.927 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:01:46.958 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:01:59.978 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:01:59.978 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:02:00.555 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:02:00.556 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:02:00.568 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:02:00.569 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:02:01.590 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:02:01.592 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data select stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true, '' ;
2020-05-28 01:02:01.604 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: ParseException line 1:90 cannot recognize input near '''' ';' '<EOF>' in constant
2020-05-28 01:02:18.247 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:02:18.247 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-81943252-60cb-4f4b-8218-7b69deb550ba sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:02:18.247 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-f7c1f4ef-9bed-4036-b53c-5984edcf3e5e sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:02:18.249 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:02:18.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-2a080205-5af9-4ea1-a885-86723d37d561 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:02:18.252 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:02:18.252 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:02:18.252 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:02:18.252 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:02:18.252 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:02:18.252 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:02:18.275 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:02:18.275 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:02:18.276 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:02:18.277 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:02:18.278 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:02:24.349 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5591 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:02:24.353 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:02:25.215 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:02:25.234 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:02:25.234 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:02:25.234 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:02:25.326 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:02:25.326 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 927 ms
2020-05-28 01:02:25.568 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:02:25.878 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:02:25.923 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:02:25.924 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:02:25.924 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598945922
2020-05-28 01:02:25.926 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:02:25.927 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:02:25.931 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:02:25.938 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:02:25.938 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:02:25.938 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598945938
2020-05-28 01:02:25.938 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:02:25.939 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:02:25.940 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:02:25.945 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:02:25.945 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:02:25.945 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590598945945
2020-05-28 01:02:25.946 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:02:25.946 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:02:25.947 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:02:25.970 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@dd71b20 via org.mortbay.log.Slf4jLog
2020-05-28 01:02:25.984 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:02:26.002 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.136 seconds (JVM running for 3.278)
2020-05-28 01:02:26.147 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:02:26.147 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:02:26.147 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:02:26.150 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:02:26.150 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:02:26.150 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:02:26.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:02:26.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:02:26.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:02:26.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:02:26.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:02:26.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:02:26.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:02:26.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:02:26.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:02:29.191 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 135: {consumer-sensors-2-609eb1be-d3f7-4d55-8bbb-ad27ed8800bd=Assignment(partitions=[]), consumer-sensors-3-cf29a7bf-75fa-4ff6-9f73-78590c6baf90=Assignment(partitions=[]), consumer-sensors-1-9bd5ce43-246a-4bf8-b6f7-1d12ebef3afb=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:02:29.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 135
2020-05-28 01:02:29.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 135
2020-05-28 01:02:29.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 135
2020-05-28 01:02:29.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:02:29.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:02:29.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:02:29.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:02:29.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:02:29.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=85, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:02:29.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:02:29.239 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:02:33.820 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:02:33.820 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:02:34.192 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:02:34.193 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:02:34.214 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:02:34.214 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:02:35.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:02:35.271 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data select stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true, '' 
2020-05-28 01:02:35.557 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 1:29 Invalid table alias or column reference 'stff000001c0c480d43abf611d3d': (possible column names are: )
2020-05-28 01:02:35.559 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:02:35.559 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:02:35.598 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:02:35.599 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:02:35.600 [Druid-ConnectionPool-Create-884379941] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:02:35.600 [Druid-ConnectionPool-Create-884379941] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:02:36.479 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data select stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true, '' 
2020-05-28 01:02:36.808 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 1:29 Invalid table alias or column reference 'stff000001c0c480d43abf611d3d': (possible column names are: )
2020-05-28 01:03:14.863 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:03:14.863 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-cf29a7bf-75fa-4ff6-9f73-78590c6baf90 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:03:14.863 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-609eb1be-d3f7-4d55-8bbb-ad27ed8800bd sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:03:14.866 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:03:14.866 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-9bd5ce43-246a-4bf8-b6f7-1d12ebef3afb sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:03:14.868 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:03:14.868 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:03:14.868 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:03:14.869 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:03:14.869 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:03:14.869 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:03:14.906 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:03:14.906 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:03:14.906 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:03:14.908 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:03:14.909 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:03:21.328 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5641 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:03:21.333 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:03:22.181 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:03:22.188 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:03:22.189 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:03:22.189 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:03:22.279 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:03:22.279 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 894 ms
2020-05-28 01:03:22.522 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:03:22.798 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:03:22.839 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:03:22.840 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:03:22.840 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599002838
2020-05-28 01:03:22.841 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:03:22.842 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:03:22.847 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:03:22.854 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:03:22.854 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:03:22.854 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599002854
2020-05-28 01:03:22.854 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:03:22.854 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:03:22.855 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:03:22.860 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:03:22.860 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:03:22.860 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599002860
2020-05-28 01:03:22.861 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:03:22.861 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:03:22.862 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:03:22.884 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@3f183caa via org.mortbay.log.Slf4jLog
2020-05-28 01:03:22.898 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:03:22.914 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.068 seconds (JVM running for 3.077)
2020-05-28 01:03:23.036 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:03:23.036 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:03:23.036 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:03:23.039 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:03:23.039 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:03:23.039 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:03:23.042 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:03:23.042 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:03:23.042 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:03:23.068 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:03:23.068 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:03:23.068 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:03:23.069 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:03:23.069 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:03:23.069 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:03:26.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 137: {consumer-sensors-2-b18e36f7-92ff-4e1a-a7e5-3aaa1d2dfaf2=Assignment(partitions=[]), consumer-sensors-3-ab559f08-8da8-400b-a38a-d5c75be3a8b5=Assignment(partitions=[]), consumer-sensors-1-57a293d1-b5e9-45e7-9984-6d4f843e8861=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:03:26.090 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 137
2020-05-28 01:03:26.090 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 137
2020-05-28 01:03:26.090 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 137
2020-05-28 01:03:26.091 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:03:26.091 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:03:26.091 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:03:26.091 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:03:26.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:03:26.104 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=87, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:03:26.104 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:03:26.127 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:03:29.892 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:03:29.892 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:03:30.337 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:03:30.337 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:03:30.359 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:03:30.360 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:03:31.371 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:03:31.372 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data select 1,stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true, '' 
2020-05-28 01:03:31.651 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 1:31 Invalid table alias or column reference 'stff000001c0c480d43abf611d3d': (possible column names are: )
2020-05-28 01:06:04.632 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:06:04.632 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-b18e36f7-92ff-4e1a-a7e5-3aaa1d2dfaf2 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:06:04.632 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-ab559f08-8da8-400b-a38a-d5c75be3a8b5 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:06:04.634 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:06:04.634 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-57a293d1-b5e9-45e7-9984-6d4f843e8861 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:06:04.635 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:06:04.635 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:06:04.635 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:06:04.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:06:04.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:06:04.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:06:04.656 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:06:04.656 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:06:04.657 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:06:04.660 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:06:04.661 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:06:10.937 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5782 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:06:10.942 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:06:11.967 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:06:11.974 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:06:11.975 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:06:11.975 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:06:12.104 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:06:12.105 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1126 ms
2020-05-28 01:06:12.346 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:06:12.666 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:06:12.712 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:06:12.712 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:06:12.712 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599172711
2020-05-28 01:06:12.713 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:06:12.715 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:06:12.718 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:06:12.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:06:12.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:06:12.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599172724
2020-05-28 01:06:12.725 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:06:12.725 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:06:12.726 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:06:12.731 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:06:12.732 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:06:12.732 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599172731
2020-05-28 01:06:12.732 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:06:12.732 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:06:12.733 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:06:12.754 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@5922d3e9 via org.mortbay.log.Slf4jLog
2020-05-28 01:06:12.767 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:06:12.784 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.269 seconds (JVM running for 3.668)
2020-05-28 01:06:12.918 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:06:12.918 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:06:12.918 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:06:12.921 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:06:12.921 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:06:12.921 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:06:12.925 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:06:12.925 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:06:12.925 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:06:12.956 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:06:12.956 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:06:12.956 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:06:12.956 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:06:12.956 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:06:12.956 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:06:15.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 139: {consumer-sensors-2-96a52c6e-15f1-4cd3-92af-d07d4a39e3f0=Assignment(partitions=[]), consumer-sensors-3-60f58ada-5624-48b1-aec2-b63049f92dbb=Assignment(partitions=[]), consumer-sensors-1-f86ab033-0f13-464c-8a4a-bae679af1435=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:06:15.982 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 139
2020-05-28 01:06:15.982 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 139
2020-05-28 01:06:15.982 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 139
2020-05-28 01:06:15.982 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:06:15.982 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:06:15.983 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:06:15.983 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:06:15.985 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:06:15.997 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=88, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:06:15.998 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:06:16.031 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:06:17.988 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-60f58ada-5624-48b1-aec2-b63049f92dbb sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:06:17.988 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:06:17.988 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-96a52c6e-15f1-4cd3-92af-d07d4a39e3f0 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:06:17.988 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:06:17.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-f86ab033-0f13-464c-8a4a-bae679af1435 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:06:17.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:06:17.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:06:17.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:06:17.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:06:17.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:06:17.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:06:18.008 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:06:18.008 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:06:18.008 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:06:18.012 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:06:24.240 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 5797 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:06:24.247 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:06:25.276 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:06:25.282 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:06:25.283 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:06:25.283 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:06:25.374 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:06:25.374 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1078 ms
2020-05-28 01:06:25.701 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:06:26.025 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:06:26.067 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:06:26.067 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:06:26.067 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599186066
2020-05-28 01:06:26.069 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:06:26.070 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:06:26.073 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:06:26.079 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:06:26.080 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:06:26.080 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599186079
2020-05-28 01:06:26.080 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:06:26.080 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:06:26.081 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:06:26.087 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:06:26.087 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:06:26.087 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599186087
2020-05-28 01:06:26.088 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:06:26.088 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:06:26.089 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:06:26.112 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@696b4a95 via org.mortbay.log.Slf4jLog
2020-05-28 01:06:26.126 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:06:26.145 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.533 seconds (JVM running for 3.685)
2020-05-28 01:06:26.260 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:06:26.260 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:06:26.260 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:06:26.261 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:06:26.261 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:06:26.261 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:06:26.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:06:26.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:06:26.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:06:26.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:06:26.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:06:26.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:06:26.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:06:26.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:06:26.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:06:29.302 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 141: {consumer-sensors-3-32f20bb3-ce0f-4c20-a4ef-df6fdafe1f80=Assignment(partitions=[]), consumer-sensors-2-b495565c-5dd2-454e-9d20-2ddf3e55d9fb=Assignment(partitions=[]), consumer-sensors-1-935c1973-cfd2-4a6b-9bb0-83a4c3ec001d=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:06:29.310 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 141
2020-05-28 01:06:29.310 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 141
2020-05-28 01:06:29.310 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 141
2020-05-28 01:06:29.311 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:06:29.311 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:06:29.311 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:06:29.311 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:06:29.315 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:06:29.330 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=88, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:06:29.331 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:06:29.358 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:06:37.125 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:06:37.125 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:06:37.588 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:06:37.589 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:06:37.604 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:06:37.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:06:38.642 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:06:38.644 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data select 1 id,stff000001c0c480d43abf611d3d distinct_id,wenWenConsultingExpert event,true login , '' properties
2020-05-28 01:06:38.922 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 1:34 Invalid table alias or column reference 'stff000001c0c480d43abf611d3d': (possible column names are: )
2020-05-28 01:10:37.890 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-32f20bb3-ce0f-4c20-a4ef-df6fdafe1f80 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:10:37.890 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:10:37.890 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-b495565c-5dd2-454e-9d20-2ddf3e55d9fb sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:10:37.893 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:10:37.893 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-935c1973-cfd2-4a6b-9bb0-83a4c3ec001d sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:10:37.895 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:10:37.895 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:10:37.895 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:10:37.895 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:10:37.895 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:10:37.895 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:10:37.918 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:10:37.919 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:10:37.920 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:10:37.925 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:10:37.930 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:10:45.780 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 6013 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:10:45.792 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:10:46.884 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:10:46.892 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:10:46.892 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:10:46.893 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:10:46.987 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:10:46.987 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1147 ms
2020-05-28 01:10:47.326 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:10:47.625 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:10:47.667 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:10:47.668 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:10:47.668 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599447667
2020-05-28 01:10:47.669 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:10:47.671 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:10:47.675 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:10:47.681 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:10:47.681 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:10:47.681 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599447681
2020-05-28 01:10:47.681 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:10:47.681 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:10:47.682 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:10:47.690 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:10:47.690 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:10:47.690 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590599447690
2020-05-28 01:10:47.691 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:10:47.691 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:10:47.692 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:10:47.714 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@36327cec via org.mortbay.log.Slf4jLog
2020-05-28 01:10:47.731 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:10:47.749 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.517 seconds (JVM running for 3.996)
2020-05-28 01:10:47.875 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:10:47.875 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:10:47.875 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:10:47.877 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:10:47.877 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:10:47.877 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:10:47.878 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:10:47.878 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:10:47.878 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:10:47.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:10:47.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:10:47.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:10:47.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:10:47.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:10:47.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:10:50.909 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 143: {consumer-sensors-1-84a47fee-1b38-4aa1-8ffd-126be5c433bc=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-a29c799b-da9c-450c-b6a9-0f2d84d173b0=Assignment(partitions=[]), consumer-sensors-2-0748bf8c-75ec-4fb1-9446-1438e797dd36=Assignment(partitions=[])}
2020-05-28 01:10:50.917 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 143
2020-05-28 01:10:50.917 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 143
2020-05-28 01:10:50.917 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 143
2020-05-28 01:10:50.918 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:10:50.918 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:10:50.919 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:10:50.919 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:10:50.922 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:10:50.936 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=89, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:10:50.936 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:10:50.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:10:55.894 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:10:55.894 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:10:56.369 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:10:56.369 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:10:56.382 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:10:56.382 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:10:57.390 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:10:57.391 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.dao.impl.SensorsHiveDaoImpl - insert into cece_data select 1 id,stff000001c0c480d43abf611d3d distinct_id,wenWenConsultingExpert event,true login , '' properties
2020-05-28 01:10:57.712 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 1:34 Invalid table alias or column reference 'stff000001c0c480d43abf611d3d': (possible column names are: )
2020-05-28 01:22:33.704 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-a29c799b-da9c-450c-b6a9-0f2d84d173b0 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:22:33.704 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-0748bf8c-75ec-4fb1-9446-1438e797dd36 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:22:33.704 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:22:33.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:22:33.709 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-84a47fee-1b38-4aa1-8ffd-126be5c433bc sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:22:33.712 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:22:33.712 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:22:33.712 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:22:33.712 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:22:33.713 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:22:33.713 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:22:33.727 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:22:33.727 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:22:33.728 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:22:33.732 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:22:33.737 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:22:41.541 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 6551 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:22:41.546 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:22:42.474 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:22:42.482 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:22:42.482 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:22:42.482 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:22:42.573 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:22:42.574 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 973 ms
2020-05-28 01:22:42.806 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:22:43.090 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:22:43.128 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:22:43.129 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:22:43.129 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600163128
2020-05-28 01:22:43.130 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:22:43.132 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:22:43.135 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:22:43.140 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:22:43.141 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:22:43.141 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600163140
2020-05-28 01:22:43.141 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:22:43.141 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:22:43.142 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:22:43.147 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:22:43.147 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:22:43.147 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600163147
2020-05-28 01:22:43.148 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:22:43.148 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:22:43.149 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:22:43.169 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@770beef5 via org.mortbay.log.Slf4jLog
2020-05-28 01:22:43.184 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:22:43.202 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.146 seconds (JVM running for 3.511)
2020-05-28 01:22:43.316 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:22:43.316 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:22:43.316 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:22:43.317 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:22:43.317 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:22:43.317 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:22:43.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:22:43.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:22:43.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:22:43.339 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:22:43.339 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:22:43.339 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:22:43.339 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:22:43.339 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:22:43.339 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:22:46.350 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 145: {consumer-sensors-3-5afa4dca-ed46-449e-9229-8d616a573975=Assignment(partitions=[]), consumer-sensors-2-7268969f-e478-45e7-ad9b-9e10b64b3a15=Assignment(partitions=[]), consumer-sensors-1-594a5a84-6e64-430d-8ad1-b677518e7428=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:22:46.359 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 145
2020-05-28 01:22:46.359 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 145
2020-05-28 01:22:46.359 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 145
2020-05-28 01:22:46.360 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:22:46.360 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:22:46.360 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:22:46.360 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:22:46.362 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:22:46.373 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=90, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:22:46.373 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:22:46.403 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:22:57.183 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:22:57.183 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:22:57.770 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:22:57.770 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:22:57.789 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:22:57.789 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:22:58.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:22:58.861 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; bad SQL grammar [insert into Table cece_data(distinct_id,event,login,properties) values(stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true,{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0})]; nested exception is org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:128 cannot recognize input near '{' '"problem_description"' ':' in expression specification
2020-05-28 01:23:20.198 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-5afa4dca-ed46-449e-9229-8d616a573975 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:23:20.198 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-7268969f-e478-45e7-ad9b-9e10b64b3a15 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:23:20.198 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:23:20.199 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:23:20.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-594a5a84-6e64-430d-8ad1-b677518e7428 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:23:20.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:23:20.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:23:20.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:23:20.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:23:20.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:23:20.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:23:20.223 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:23:20.224 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:23:20.225 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:23:20.228 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:23:20.266 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:23:26.216 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 6595 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:23:26.220 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:23:27.073 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:23:27.080 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:23:27.080 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:23:27.080 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:23:27.163 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:23:27.164 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 905 ms
2020-05-28 01:23:27.442 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:23:27.739 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:23:27.779 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:23:27.779 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:23:27.779 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600207778
2020-05-28 01:23:27.781 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:23:27.782 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:23:27.785 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:23:27.792 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:23:27.793 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:23:27.793 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600207792
2020-05-28 01:23:27.793 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:23:27.794 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:23:27.795 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:23:27.801 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:23:27.801 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:23:27.801 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600207801
2020-05-28 01:23:27.801 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:23:27.801 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:23:27.802 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:23:27.822 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@7343922c via org.mortbay.log.Slf4jLog
2020-05-28 01:23:27.835 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:23:27.851 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.179 seconds (JVM running for 3.19)
2020-05-28 01:23:27.959 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:23:27.959 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:23:27.959 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:23:27.961 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:23:27.961 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:23:27.961 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:23:27.964 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:23:27.964 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:23:27.964 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:23:27.988 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:23:27.988 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:23:27.988 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:23:27.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:23:27.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:23:27.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:23:31.001 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 147: {consumer-sensors-2-853a59f9-2f01-4618-b988-2934f08fa99e=Assignment(partitions=[]), consumer-sensors-1-52e3c065-02d6-4c5a-b426-574fa0dfd7bc=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-c9bff116-0512-4f45-b56d-edafe8168406=Assignment(partitions=[])}
2020-05-28 01:23:31.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 147
2020-05-28 01:23:31.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 147
2020-05-28 01:23:31.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 147
2020-05-28 01:23:31.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:23:31.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:23:31.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:23:31.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:23:31.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:23:31.029 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=91, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:23:31.030 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:23:31.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:23:34.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:23:34.649 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:23:35.207 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:23:35.207 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:23:35.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:23:35.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:23:36.278 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:23:36.350 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; bad SQL grammar [insert into Table cece_data(distinct_id,event,login,properties) values(stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true,{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0})]; nested exception is org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:128 cannot recognize input near '{' '"problem_description"' ':' in expression specification
2020-05-28 01:24:13.844 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-853a59f9-2f01-4618-b988-2934f08fa99e sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:24:13.844 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-c9bff116-0512-4f45-b56d-edafe8168406 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:24:13.844 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:24:13.846 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:24:13.846 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-52e3c065-02d6-4c5a-b426-574fa0dfd7bc sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:24:13.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:24:13.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:24:13.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:24:13.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:24:13.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:24:13.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:24:13.860 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:24:13.860 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:24:13.860 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:24:13.864 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:24:13.889 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:24:19.783 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 6640 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:24:19.787 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:24:20.811 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:24:20.819 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:24:20.819 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:24:20.820 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:24:20.936 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:24:20.936 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1121 ms
2020-05-28 01:24:21.210 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:24:21.537 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:24:21.582 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:24:21.582 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:24:21.582 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600261581
2020-05-28 01:24:21.583 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:24:21.585 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:24:21.588 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:24:21.595 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:24:21.596 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:24:21.596 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600261595
2020-05-28 01:24:21.596 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:24:21.596 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:24:21.597 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:24:21.603 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:24:21.603 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:24:21.603 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600261603
2020-05-28 01:24:21.604 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:24:21.604 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:24:21.605 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:24:21.627 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@1cdc1bbc via org.mortbay.log.Slf4jLog
2020-05-28 01:24:21.640 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:24:21.657 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.27 seconds (JVM running for 3.265)
2020-05-28 01:24:21.782 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:24:21.782 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:24:21.782 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:24:21.784 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:24:21.784 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:24:21.784 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:24:21.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:24:21.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:24:21.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:24:21.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:24:21.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:24:21.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:24:21.813 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:24:21.813 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:24:21.813 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:24:24.824 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 149: {consumer-sensors-2-a263cb8a-f276-4c4e-8903-4bb40dc23588=Assignment(partitions=[]), consumer-sensors-1-be1f06d6-4498-48e5-8b6d-5ff4768557bf=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-b8e1d004-b457-45bc-9b60-96e7ec528202=Assignment(partitions=[])}
2020-05-28 01:24:24.833 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 149
2020-05-28 01:24:24.833 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 149
2020-05-28 01:24:24.833 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 149
2020-05-28 01:24:24.833 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:24:24.833 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:24:24.834 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:24:24.834 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:24:24.836 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:24:24.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=92, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:24:24.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:24:24.875 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:24:31.522 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:24:31.524 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:24:32.046 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:24:32.046 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:24:32.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:24:32.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:24:33.114 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:24:33.418 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; bad SQL grammar [insert into Table cece_data(distinct_id,event,login,properties) values(stff000001c0c480d43abf611d3d,wenWenConsultingExpert,true,test)]; nested exception is org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_TABLE_OR_COL not supported in insert/values
2020-05-28 01:31:09.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-a263cb8a-f276-4c4e-8903-4bb40dc23588 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:31:09.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:31:09.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-b8e1d004-b457-45bc-9b60-96e7ec528202 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:31:09.252 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:31:09.252 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-be1f06d6-4498-48e5-8b6d-5ff4768557bf sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:31:09.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:31:09.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:31:09.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:31:09.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:31:09.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:31:09.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:31:09.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:31:09.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:31:09.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:31:09.275 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:31:09.317 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:31:16.233 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 6980 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:31:16.237 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:31:17.142 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:31:17.150 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:31:17.150 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:31:17.150 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:31:17.259 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:31:17.259 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 989 ms
2020-05-28 01:31:17.502 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:31:17.881 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:31:17.925 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:31:17.925 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:31:17.925 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600677924
2020-05-28 01:31:17.927 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:31:17.928 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:31:17.931 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:31:17.938 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:31:17.938 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:31:17.938 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600677937
2020-05-28 01:31:17.938 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:31:17.938 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:31:17.939 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:31:17.945 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:31:17.945 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:31:17.945 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600677945
2020-05-28 01:31:17.946 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:31:17.946 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:31:17.947 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:31:17.972 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@1921994e via org.mortbay.log.Slf4jLog
2020-05-28 01:31:17.992 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:31:18.013 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.251 seconds (JVM running for 3.812)
2020-05-28 01:31:18.146 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:31:18.146 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:31:18.146 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:31:18.149 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:31:18.149 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:31:18.149 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:31:18.152 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:31:18.152 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:31:18.152 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:31:18.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:31:18.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:31:18.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:31:18.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:31:18.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:31:18.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:31:21.186 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 151: {consumer-sensors-3-b2fc9fd2-691d-4581-adf4-8db3aa13ca4f=Assignment(partitions=[]), consumer-sensors-1-6704081a-7084-41b9-872e-80053d658761=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-eda0b691-4bdc-4bb8-b755-18429b99a0f9=Assignment(partitions=[])}
2020-05-28 01:31:21.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 151
2020-05-28 01:31:21.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 151
2020-05-28 01:31:21.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 151
2020-05-28 01:31:21.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:31:21.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:31:21.195 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:31:21.195 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:31:21.197 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:31:21.208 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=93, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:31:21.208 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:31:21.239 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:31:35.691 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:31:35.691 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:31:36.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:31:36.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:31:36.197 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:31:36.197 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:31:37.245 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:31:37.331 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; bad SQL grammar [insert into Table cece_data(distinct_id,event,login,properties) values("stff000001c0c480d43abf611d3d","wenWenConsultingExpert","true","{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}")]; nested exception is org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:137 mismatched input 'problem_description' expecting ) near '"{"' in value row constructor
2020-05-28 01:31:57.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:31:57.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-eda0b691-4bdc-4bb8-b755-18429b99a0f9 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:31:57.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-b2fc9fd2-691d-4581-adf4-8db3aa13ca4f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:31:57.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:31:57.297 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-6704081a-7084-41b9-872e-80053d658761 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:31:57.298 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:31:57.298 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:31:57.298 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:31:57.298 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:31:57.298 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:31:57.298 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:31:57.311 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:31:57.315 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:31:57.315 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:31:57.317 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:31:57.347 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:32:03.566 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7025 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:32:03.570 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:32:04.571 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:32:04.578 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:32:04.579 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:32:04.579 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:32:04.683 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:32:04.683 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1079 ms
2020-05-28 01:32:04.981 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:32:05.267 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:32:05.307 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:32:05.307 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:32:05.307 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600725306
2020-05-28 01:32:05.309 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:32:05.310 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:32:05.314 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:32:05.321 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:32:05.321 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:32:05.321 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600725321
2020-05-28 01:32:05.322 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:32:05.322 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:32:05.324 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:32:05.329 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:32:05.329 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:32:05.329 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600725329
2020-05-28 01:32:05.330 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:32:05.330 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:32:05.331 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:32:05.354 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@6f9c5048 via org.mortbay.log.Slf4jLog
2020-05-28 01:32:05.366 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:32:05.383 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.283 seconds (JVM running for 3.518)
2020-05-28 01:32:05.501 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:32:05.501 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:32:05.501 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:32:05.504 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:32:05.504 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:32:05.504 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:32:05.507 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:32:05.507 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:32:05.507 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:32:05.532 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:32:05.532 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:32:05.532 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:32:05.533 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:32:05.533 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:32:05.533 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:32:08.544 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 153: {consumer-sensors-3-d712a299-dd2e-4896-b3a7-af74dc02739b=Assignment(partitions=[]), consumer-sensors-2-6249a907-aeb1-4ef5-aa3c-0466cd579b8a=Assignment(partitions=[]), consumer-sensors-1-1ee5ceeb-547b-4d95-82ab-2d16fc4dfad7=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:32:08.553 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 153
2020-05-28 01:32:08.553 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 153
2020-05-28 01:32:08.553 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 153
2020-05-28 01:32:08.553 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:32:08.553 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:32:08.553 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:32:08.553 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:32:08.555 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:32:08.567 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=94, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:32:08.567 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:32:08.599 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:32:12.373 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:32:12.373 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:32:12.846 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:32:12.846 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:32:12.875 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:32:12.876 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:32:13.847 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:32:44.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into Table cece_data(distinct_id,event,login,properties) values("stff000001c0c480d43abf611d3d","wenWenConsultingExpert","true",'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}')]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 01:34:45.050 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-d712a299-dd2e-4896-b3a7-af74dc02739b sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:34:45.050 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-6249a907-aeb1-4ef5-aa3c-0466cd579b8a sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:34:45.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:34:45.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:34:45.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:34:45.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:34:45.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:34:45.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-1ee5ceeb-547b-4d95-82ab-2d16fc4dfad7 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:34:45.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:34:45.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:34:45.055 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:34:45.076 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:34:45.078 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:34:45.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:34:45.083 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:34:45.152 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:34:52.083 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7160 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:34:52.088 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:34:53.068 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:34:53.076 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:34:53.077 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:34:53.077 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:34:53.205 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:34:53.206 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1061 ms
2020-05-28 01:34:53.508 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:34:53.887 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:34:53.944 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:34:53.944 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:34:53.945 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600893943
2020-05-28 01:34:53.947 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:34:53.949 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:34:53.954 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:34:53.965 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:34:53.965 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:34:53.965 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600893965
2020-05-28 01:34:53.966 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:34:53.966 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:34:53.967 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:34:53.975 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:34:53.975 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:34:53.975 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590600893975
2020-05-28 01:34:53.976 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:34:53.976 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:34:53.977 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:34:54.006 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@35f3a22c via org.mortbay.log.Slf4jLog
2020-05-28 01:34:54.023 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:34:54.050 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.492 seconds (JVM running for 3.982)
2020-05-28 01:34:54.236 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:34:54.236 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:34:54.236 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:34:54.240 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:34:54.240 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:34:54.240 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:34:54.244 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:34:54.244 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:34:54.244 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:34:54.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:34:54.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:34:54.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:34:54.271 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:34:54.271 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:34:54.271 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:34:57.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 155: {consumer-sensors-3-ebfc3cfc-8b56-4ec5-a6cb-b4442ee417b9=Assignment(partitions=[]), consumer-sensors-2-04971452-81dc-4c03-a4e2-3989c4f1f725=Assignment(partitions=[]), consumer-sensors-1-e6b530ea-48cb-4de4-a961-6f5d4135a5d4=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:34:57.292 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 155
2020-05-28 01:34:57.292 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 155
2020-05-28 01:34:57.292 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 155
2020-05-28 01:34:57.293 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:34:57.293 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:34:57.293 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:34:57.293 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:34:57.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:34:57.307 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=95, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:34:57.307 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:34:57.335 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:35:02.651 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:35:02.652 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:35:03.115 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:35:03.115 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:35:03.133 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:35:03.133 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:35:04.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:35:31.226 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:36:09.355 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:36:09.356 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:36:09.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:36:09.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:36:29.482 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into Table cece_data(distinct_id,event,login,properties) values('stff000001c0c480d43abf611d3d','wenWenConsultingExpert',false,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}')]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 01:36:38.038 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:36:38.038 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:36:38.088 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:36:38.088 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:36:47.801 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:38:01.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:38:01.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:38:01.744 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:38:01.744 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:38:11.595 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:40:33.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:40:33.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-04971452-81dc-4c03-a4e2-3989c4f1f725 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:40:33.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-ebfc3cfc-8b56-4ec5-a6cb-b4442ee417b9 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:40:33.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:40:33.647 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-e6b530ea-48cb-4de4-a961-6f5d4135a5d4 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:40:33.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:40:33.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:40:33.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:40:33.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:40:33.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:40:33.648 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:40:33.671 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:40:33.670 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:40:33.670 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:40:33.677 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:40:33.793 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:40:41.111 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7428 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:40:41.117 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:40:43.804 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:40:43.816 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:40:43.816 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:40:43.817 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:40:44.035 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:40:44.035 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 2877 ms
2020-05-28 01:40:44.736 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:40:45.292 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:40:45.355 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:40:45.355 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:40:45.355 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601245354
2020-05-28 01:40:45.358 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:40:45.361 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:40:45.371 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:40:45.382 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:40:45.382 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:40:45.382 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601245382
2020-05-28 01:40:45.383 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:40:45.383 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:40:45.387 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:40:45.399 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:40:45.399 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:40:45.399 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601245399
2020-05-28 01:40:45.400 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:40:45.400 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:40:45.402 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:40:45.435 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@51b01550 via org.mortbay.log.Slf4jLog
2020-05-28 01:40:45.454 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:40:45.481 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 5.016 seconds (JVM running for 6.533)
2020-05-28 01:40:45.699 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:40:45.699 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:40:45.699 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:40:45.702 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:40:45.702 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:40:45.702 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:40:45.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:40:45.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:40:45.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:40:45.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:40:45.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:40:45.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:40:45.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:40:45.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:40:45.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:40:48.743 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 157: {consumer-sensors-3-1c2343a3-e898-409e-abcf-fefda2e9733c=Assignment(partitions=[]), consumer-sensors-2-78e6220e-132c-4b35-a6f4-5c2ca8a7b06a=Assignment(partitions=[]), consumer-sensors-1-d8f0ccf5-2679-4788-83f1-72311c54c608=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:40:48.755 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 157
2020-05-28 01:40:48.755 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 157
2020-05-28 01:40:48.755 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 157
2020-05-28 01:40:48.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:40:48.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:40:48.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:40:48.756 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:40:48.758 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:40:48.769 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=99, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:40:48.770 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:40:48.800 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:40:53.545 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:40:53.545 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:40:54.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:40:54.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:40:54.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:40:54.060 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:40:55.083 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:40:55.375 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; bad SQL grammar [insert into Table cece_data(id,distinct_id,event,login,properties) values(row_number(),'stff000001c0c480d43abf611d3d','wenWenConsultingExpert',false,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}')]; nested exception is org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_FUNCTION not supported in insert/values
2020-05-28 01:42:58.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-1c2343a3-e898-409e-abcf-fefda2e9733c sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:42:58.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:42:58.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-78e6220e-132c-4b35-a6f4-5c2ca8a7b06a sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:42:58.709 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:42:58.709 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-d8f0ccf5-2679-4788-83f1-72311c54c608 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:42:58.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:42:58.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:42:58.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:42:58.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:42:58.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:42:58.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:42:58.729 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:42:58.729 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:42:58.730 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:42:58.733 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:42:58.764 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:43:05.693 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7542 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:43:05.697 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:43:06.649 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:43:06.657 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:43:06.658 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:43:06.658 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:43:06.752 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:43:06.752 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1021 ms
2020-05-28 01:43:07.032 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:43:07.368 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:43:07.409 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:43:07.409 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:43:07.409 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601387408
2020-05-28 01:43:07.411 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:43:07.412 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:43:07.416 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:43:07.422 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:43:07.422 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:43:07.423 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601387422
2020-05-28 01:43:07.423 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:43:07.423 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:43:07.424 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:43:07.430 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:43:07.430 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:43:07.430 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601387430
2020-05-28 01:43:07.430 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:43:07.431 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:43:07.432 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:43:07.453 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@4779aae6 via org.mortbay.log.Slf4jLog
2020-05-28 01:43:07.467 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:43:07.484 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.247 seconds (JVM running for 3.687)
2020-05-28 01:43:07.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:43:07.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:43:07.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:43:07.604 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:43:07.604 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:43:07.604 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:43:07.608 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:43:07.608 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:43:07.608 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:43:07.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:43:07.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:43:07.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:43:07.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:43:07.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:43:07.636 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:43:10.650 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 159: {consumer-sensors-3-a0470ef9-4b19-4a5d-8445-a547a9369dd5=Assignment(partitions=[]), consumer-sensors-1-00f50350-c803-42f8-8c77-5b5c4f451ac0=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-6602d02d-bcad-417b-aaa8-705533a96b60=Assignment(partitions=[])}
2020-05-28 01:43:10.660 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 159
2020-05-28 01:43:10.660 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 159
2020-05-28 01:43:10.660 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 159
2020-05-28 01:43:10.660 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:43:10.660 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:43:10.661 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:43:10.661 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:43:10.663 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:43:10.676 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=100, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:43:10.676 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:43:10.700 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:43:15.945 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:43:15.945 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:43:16.442 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:43:16.442 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:43:16.459 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:43:16.459 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:43:17.503 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:43:52.678 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:43:56.198 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:43:56.198 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-6602d02d-bcad-417b-aaa8-705533a96b60 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:43:56.198 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-a0470ef9-4b19-4a5d-8445-a547a9369dd5 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:43:56.199 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:43:56.199 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-00f50350-c803-42f8-8c77-5b5c4f451ac0 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:43:56.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:43:56.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:43:56.200 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:43:56.201 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:43:56.201 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:43:56.201 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:43:56.216 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:43:56.217 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:43:56.217 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:43:56.219 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:43:56.272 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:44:02.705 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7593 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:44:02.710 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:44:03.771 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:44:03.778 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:44:03.779 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:44:03.779 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:44:03.867 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:44:03.867 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1116 ms
2020-05-28 01:44:04.135 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:44:04.447 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:44:04.493 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:44:04.493 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:44:04.493 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601444492
2020-05-28 01:44:04.495 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:44:04.496 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:44:04.500 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:44:04.507 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:44:04.507 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:44:04.507 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601444507
2020-05-28 01:44:04.507 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:44:04.507 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:44:04.508 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:44:04.513 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:44:04.513 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:44:04.514 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601444513
2020-05-28 01:44:04.514 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:44:04.514 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:44:04.515 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:44:04.535 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@2f9a4401 via org.mortbay.log.Slf4jLog
2020-05-28 01:44:04.550 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:44:04.567 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.386 seconds (JVM running for 3.413)
2020-05-28 01:44:04.676 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:44:04.676 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:44:04.676 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:44:04.677 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:44:04.677 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:44:04.678 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:44:04.681 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:44:04.681 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:44:04.681 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:44:04.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:44:04.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:44:04.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:44:04.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:44:04.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:44:04.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:44:07.722 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 161: {consumer-sensors-2-17168c45-6306-441e-9b87-40c6ba69b3e9=Assignment(partitions=[]), consumer-sensors-1-642a3fe4-9be8-405e-aa58-40d588b1148f=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-011bac1e-6b63-406e-8431-ecdd97ed9bf2=Assignment(partitions=[])}
2020-05-28 01:44:07.732 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 161
2020-05-28 01:44:07.732 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 161
2020-05-28 01:44:07.732 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 161
2020-05-28 01:44:07.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:44:07.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:44:07.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:44:07.733 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:44:07.734 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:44:07.746 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=101, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:44:07.747 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:44:07.779 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:44:11.941 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:44:11.941 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:44:12.389 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:44:12.390 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:44:12.407 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:44:12.407 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:44:13.414 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:44:42.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into Table cece_data(distinct_id,event,login,properties) values('stff000001c0c480d43abf611d3d','wenWenConsultingExpert',false,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}')]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 01:44:42.229 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:44:42.230 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:44:42.326 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:44:42.326 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:44:44.933 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:44:52.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-17168c45-6306-441e-9b87-40c6ba69b3e9 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:44:52.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:44:52.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-011bac1e-6b63-406e-8431-ecdd97ed9bf2 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:44:52.064 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:44:52.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-642a3fe4-9be8-405e-aa58-40d588b1148f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:44:52.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:44:52.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:44:52.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:44:52.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:44:52.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:44:52.066 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:44:52.116 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:44:52.117 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:44:52.118 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:44:52.119 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:44:52.178 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:44:58.206 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7642 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:44:58.210 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:44:59.077 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:44:59.084 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:44:59.085 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:44:59.085 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:44:59.174 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:44:59.174 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 929 ms
2020-05-28 01:44:59.425 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:44:59.737 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:44:59.784 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:44:59.784 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:44:59.784 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601499783
2020-05-28 01:44:59.785 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:44:59.787 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:44:59.790 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:44:59.796 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:44:59.796 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:44:59.796 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601499796
2020-05-28 01:44:59.796 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:44:59.796 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:44:59.798 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:44:59.803 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:44:59.803 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:44:59.803 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601499803
2020-05-28 01:44:59.803 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:44:59.803 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:44:59.804 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:44:59.825 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@593a6726 via org.mortbay.log.Slf4jLog
2020-05-28 01:44:59.837 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:44:59.854 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.139 seconds (JVM running for 3.253)
2020-05-28 01:44:59.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:44:59.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:44:59.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:44:59.968 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:44:59.968 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:44:59.968 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:44:59.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:44:59.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:44:59.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:44:59.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:44:59.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:44:59.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:44:59.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:44:59.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:44:59.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:45:03.003 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 163: {consumer-sensors-2-f3cd96f4-46cd-43a1-b184-cc260e8ebc98=Assignment(partitions=[]), consumer-sensors-1-fbcc2ab3-c49d-4b9f-a0c5-c6f19a5feaf7=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-b9ad9148-90e7-4b69-a1a2-804fbafaa12c=Assignment(partitions=[])}
2020-05-28 01:45:03.016 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 163
2020-05-28 01:45:03.016 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 163
2020-05-28 01:45:03.016 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 163
2020-05-28 01:45:03.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:45:03.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:45:03.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:45:03.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:45:03.019 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:45:03.034 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=103, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:45:03.035 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:45:03.061 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:45:07.092 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:45:07.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:45:07.591 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:45:07.592 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:45:07.610 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:45:07.610 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:45:08.603 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:45:38.024 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:47:47.291 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-f3cd96f4-46cd-43a1-b184-cc260e8ebc98 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:47:47.291 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-b9ad9148-90e7-4b69-a1a2-804fbafaa12c sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:47:47.291 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:47:47.294 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:47:47.294 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-fbcc2ab3-c49d-4b9f-a0c5-c6f19a5feaf7 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:47:47.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:47:47.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:47:47.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:47:47.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:47:47.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:47:47.296 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:47:47.316 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:47:47.315 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:47:47.315 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:47:47.319 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:47:47.391 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:47:57.284 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7787 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:47:57.290 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:47:59.095 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:47:59.103 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:47:59.103 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:47:59.104 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:47:59.250 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:47:59.251 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1917 ms
2020-05-28 01:47:59.535 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:47:59.897 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:47:59.940 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:47:59.941 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:47:59.941 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601679940
2020-05-28 01:47:59.942 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:47:59.944 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:47:59.947 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:47:59.954 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:47:59.954 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:47:59.954 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601679954
2020-05-28 01:47:59.954 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:47:59.955 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:47:59.956 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:47:59.962 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:47:59.962 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:47:59.962 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601679962
2020-05-28 01:47:59.962 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:47:59.962 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:47:59.964 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:47:59.988 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@31e2232f via org.mortbay.log.Slf4jLog
2020-05-28 01:48:00.003 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:48:00.024 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 3.491 seconds (JVM running for 5.706)
2020-05-28 01:48:00.162 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:48:00.162 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:48:00.162 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:48:00.166 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:48:00.166 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:48:00.166 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:48:00.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:48:00.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:48:00.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:48:00.201 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:48:00.201 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:48:00.201 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:48:00.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:48:00.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:48:00.202 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:48:03.213 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 165: {consumer-sensors-3-d4084dff-93b8-46a3-841a-596bf3ecee79=Assignment(partitions=[]), consumer-sensors-2-61fb9a02-34c2-486e-830a-63e2babe1799=Assignment(partitions=[]), consumer-sensors-1-32c3ad2f-0c9e-4c44-8699-e5942f85c039=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:48:03.221 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 165
2020-05-28 01:48:03.221 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 165
2020-05-28 01:48:03.221 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 165
2020-05-28 01:48:03.222 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:48:03.222 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:48:03.223 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:48:03.223 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:48:03.226 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:48:03.242 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=104, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:48:03.243 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:48:03.275 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:48:17.395 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-d4084dff-93b8-46a3-841a-596bf3ecee79 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:48:17.395 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:48:17.395 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-61fb9a02-34c2-486e-830a-63e2babe1799 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:48:17.396 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:48:17.396 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-32c3ad2f-0c9e-4c44-8699-e5942f85c039 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:48:17.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:48:17.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:48:17.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:48:17.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:48:17.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:48:17.400 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:48:17.427 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:48:17.427 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:48:17.427 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:48:17.435 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:48:24.716 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7814 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:48:24.722 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:48:25.585 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:48:25.593 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:48:25.593 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:48:25.594 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:48:25.700 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:48:25.701 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 943 ms
2020-05-28 01:48:25.974 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:48:26.281 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:48:26.322 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:48:26.323 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:48:26.323 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601706322
2020-05-28 01:48:26.324 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:48:26.325 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:48:26.329 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:48:26.334 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:48:26.335 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:48:26.335 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601706334
2020-05-28 01:48:26.335 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:48:26.335 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:48:26.336 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:48:26.341 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:48:26.341 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:48:26.341 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601706341
2020-05-28 01:48:26.342 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:48:26.342 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:48:26.343 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:48:26.363 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@2dfb885e via org.mortbay.log.Slf4jLog
2020-05-28 01:48:26.376 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:48:26.394 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.224 seconds (JVM running for 3.357)
2020-05-28 01:48:26.508 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:48:26.508 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:48:26.508 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:48:26.511 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:48:26.511 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:48:26.511 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:48:26.514 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:48:26.514 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:48:26.514 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:48:26.542 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:48:26.542 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:48:26.542 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:48:26.544 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:48:26.544 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:48:26.545 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:48:29.561 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 167: {consumer-sensors-2-a6c8a009-f0e9-4023-b9c6-406a44b69e98=Assignment(partitions=[]), consumer-sensors-1-2bd1fccf-c06e-4e04-b2ef-dbd8e9ebf320=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-b4b2b7f9-05ef-42ab-a426-5ce7cc4f8d0d=Assignment(partitions=[])}
2020-05-28 01:48:29.570 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 167
2020-05-28 01:48:29.570 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 167
2020-05-28 01:48:29.570 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 167
2020-05-28 01:48:29.570 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:48:29.571 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:48:29.572 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:48:29.573 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:48:29.577 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:48:29.594 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=104, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:48:29.595 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:48:29.629 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:48:37.163 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:48:37.163 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:48:37.530 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:48:37.548 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:48:37.548 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:48:38.542 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:49:08.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:49:08.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:50:45.463 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-b4b2b7f9-05ef-42ab-a426-5ce7cc4f8d0d sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:50:45.463 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:50:45.463 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-a6c8a009-f0e9-4023-b9c6-406a44b69e98 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:50:45.465 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:50:45.466 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-2bd1fccf-c06e-4e04-b2ef-dbd8e9ebf320 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:50:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:50:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:50:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:50:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:50:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:50:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:50:45.488 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:50:45.488 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:50:45.488 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:50:45.492 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:50:45.565 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:50:53.393 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 7927 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:50:53.397 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:50:54.323 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:50:54.330 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:50:54.331 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:50:54.331 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:50:54.431 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:50:54.431 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 996 ms
2020-05-28 01:50:54.689 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:50:54.994 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:50:55.041 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:50:55.041 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:50:55.041 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601855040
2020-05-28 01:50:55.043 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:50:55.044 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:50:55.047 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:50:55.053 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:50:55.053 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:50:55.053 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601855053
2020-05-28 01:50:55.054 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:50:55.054 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:50:55.055 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:50:55.060 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:50:55.060 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:50:55.060 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601855060
2020-05-28 01:50:55.060 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:50:55.061 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:50:55.061 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:50:55.082 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@4b5a078a via org.mortbay.log.Slf4jLog
2020-05-28 01:50:55.095 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:50:55.112 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.202 seconds (JVM running for 3.433)
2020-05-28 01:50:55.230 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:50:55.230 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:50:55.230 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:50:55.233 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:50:55.233 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:50:55.233 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:50:55.236 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:50:55.236 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:50:55.236 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:50:55.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:50:55.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:50:55.263 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:50:55.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:50:55.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:50:55.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:50:58.278 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 169: {consumer-sensors-2-8e9bda07-f0b4-4928-ac82-2876c64b8134=Assignment(partitions=[]), consumer-sensors-3-0f8963b7-1ae5-4ede-b2c8-64bdb4073f1c=Assignment(partitions=[]), consumer-sensors-1-be20b68c-1ddb-4851-a73d-9f06cead4308=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 01:50:58.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 169
2020-05-28 01:50:58.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 169
2020-05-28 01:50:58.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 169
2020-05-28 01:50:58.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:50:58.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:50:58.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:50:58.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:50:58.291 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:50:58.308 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=105, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:50:58.309 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:50:58.345 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:51:01.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:51:01.917 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:51:02.322 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:51:02.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 01:51:02.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:51:02.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:51:02.342 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:51:02.343 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:51:03.322 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:51:30.277 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into Table cece_data(distinct_id,event,login,properties) values('stff000001c0c480d43abf611d3d','wenWenConsultingExpert',false,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}')]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 01:51:30.277 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 01:51:54.345 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Offset commit failed on partition cece_sensors-0 at offset 106: The request timed out.
2020-05-28 01:51:54.346 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 01:51:54.621 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:52:09.587 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:52:09.588 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:52:09.631 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:52:09.632 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 01:52:09.632 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:52:09.632 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:52:23.188 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:52:23.188 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 01:53:02.325 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-8e9bda07-f0b4-4928-ac82-2876c64b8134 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:53:02.325 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-0f8963b7-1ae5-4ede-b2c8-64bdb4073f1c sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:53:02.325 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:53:02.326 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:53:02.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-be20b68c-1ddb-4851-a73d-9f06cead4308 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:53:02.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:53:02.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:53:02.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:53:02.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:53:02.328 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:53:02.328 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:53:02.340 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:53:02.340 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:53:02.341 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:53:02.342 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:53:02.419 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:53:08.362 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 8033 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:53:08.370 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:53:09.473 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:53:09.480 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:53:09.480 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:53:09.480 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:53:09.564 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:53:09.564 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1152 ms
2020-05-28 01:53:09.840 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:53:10.144 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:53:10.183 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:53:10.183 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:53:10.184 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601990182
2020-05-28 01:53:10.185 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:53:10.186 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:53:10.190 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:53:10.195 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:53:10.196 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:53:10.196 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601990195
2020-05-28 01:53:10.196 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:53:10.196 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:53:10.197 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:53:10.202 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:53:10.202 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:53:10.202 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590601990202
2020-05-28 01:53:10.203 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:53:10.203 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:53:10.204 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:53:10.225 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@58fa5769 via org.mortbay.log.Slf4jLog
2020-05-28 01:53:10.239 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:53:10.258 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.368 seconds (JVM running for 3.557)
2020-05-28 01:53:10.374 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:53:10.374 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:53:10.374 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:53:10.377 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:53:10.377 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:53:10.377 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:53:10.381 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:53:10.381 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:53:10.381 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:53:10.411 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:53:10.411 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:53:10.411 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:53:10.411 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:53:10.411 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:53:10.411 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:53:13.423 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 171: {consumer-sensors-3-138e904f-271e-44df-a070-e8cc9c4ea336=Assignment(partitions=[]), consumer-sensors-1-331ad38b-a8c1-4c75-9c9b-8ac3a3571eaa=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-7350f52c-eab1-407d-a5fc-ce63cb47b63c=Assignment(partitions=[])}
2020-05-28 01:53:13.431 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 171
2020-05-28 01:53:13.431 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 171
2020-05-28 01:53:13.431 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 171
2020-05-28 01:53:13.431 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:53:13.431 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:53:13.432 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:53:13.432 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:53:13.433 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:53:13.446 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=107, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:53:13.447 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:53:13.477 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:53:16.848 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:53:16.848 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:53:17.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:53:17.291 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 01:53:17.291 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:53:17.292 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:53:17.304 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:53:17.304 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:53:18.298 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:53:43.234 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:53:43.236 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 01:53:43.502 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:53:43.502 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:53:43.543 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:53:43.543 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 01:53:43.543 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:53:43.543 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:53:45.715 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:53:45.715 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 01:53:53.471 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:53:53.471 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:53:53.511 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:53:53.511 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 01:53:53.511 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:53:53.511 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:54:01.572 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:54:01.574 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 01:54:01.583 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:54:01.583 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:54:01.632 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:54:01.633 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 01:54:01.633 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:54:01.633 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:54:03.587 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:54:03.587 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 01:54:26.829 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:54:26.829 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-138e904f-271e-44df-a070-e8cc9c4ea336 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:54:26.829 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-7350f52c-eab1-407d-a5fc-ce63cb47b63c sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:54:26.831 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:54:26.832 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-331ad38b-a8c1-4c75-9c9b-8ac3a3571eaa sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:54:26.835 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:54:26.835 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:54:26.835 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:54:26.835 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:54:26.835 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:54:26.835 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:54:26.855 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:54:26.856 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:54:26.857 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:54:26.860 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:54:26.923 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:54:33.153 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 8104 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:54:33.159 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:54:34.102 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:54:34.108 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:54:34.109 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:54:34.109 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:54:34.193 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:54:34.193 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 976 ms
2020-05-28 01:54:34.464 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:54:34.774 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:54:34.814 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:54:34.814 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:54:34.814 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602074813
2020-05-28 01:54:34.816 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:54:34.817 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:54:34.820 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:54:34.826 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:54:34.826 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:54:34.827 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602074826
2020-05-28 01:54:34.827 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:54:34.827 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:54:34.828 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:54:34.834 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:54:34.834 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:54:34.834 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602074834
2020-05-28 01:54:34.835 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:54:34.835 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:54:34.836 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:54:34.860 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@788ba63e via org.mortbay.log.Slf4jLog
2020-05-28 01:54:34.875 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:54:34.892 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.211 seconds (JVM running for 3.208)
2020-05-28 01:54:35.007 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:54:35.008 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:54:35.007 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:54:35.010 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:54:35.010 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:54:35.010 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:54:35.014 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:54:35.014 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:54:35.014 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:54:35.037 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:54:35.037 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:54:35.037 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:54:35.038 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:54:35.038 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:54:35.038 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:54:38.050 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 173: {consumer-sensors-2-3274e438-1ed2-41db-ab6b-f5cac600d4e9=Assignment(partitions=[]), consumer-sensors-1-f7080019-c393-4620-b744-14f770724620=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-d4c41ff9-b35e-401a-a15a-32cb208ac5ee=Assignment(partitions=[])}
2020-05-28 01:54:38.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 173
2020-05-28 01:54:38.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 173
2020-05-28 01:54:38.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 173
2020-05-28 01:54:38.060 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:54:38.060 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:54:38.060 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:54:38.060 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:54:38.062 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:54:38.074 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=111, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:54:38.075 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:54:38.102 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:54:43.720 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:54:43.720 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:54:44.158 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:54:44.167 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 01:54:44.167 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:54:44.167 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:54:44.184 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:54:44.184 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:54:45.580 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:55:13.691 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into Table cece_data(distinct_id,event,login,properties) values('stff000001c0c480d43abf611d3d','wenWenConsultingExpert',false,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}')]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 01:55:13.693 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 01:55:45.291 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:55:45.291 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:55:45.345 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:55:45.346 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: null
2020-05-28 01:55:45.346 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:55:45.346 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:55:50.472 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:55:50.472 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-d4c41ff9-b35e-401a-a15a-32cb208ac5ee sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:55:50.472 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-3274e438-1ed2-41db-ab6b-f5cac600d4e9 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:55:50.472 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:55:50.473 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-f7080019-c393-4620-b744-14f770724620 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:55:50.474 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:55:50.474 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:55:50.474 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:55:50.474 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:55:50.474 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:55:50.474 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:55:50.491 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:55:50.491 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:55:50.492 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:55:50.496 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:55:50.499 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:55:56.991 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 8174 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:55:56.997 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:55:57.939 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:55:57.946 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:55:57.947 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:55:57.947 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:55:58.033 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:55:58.034 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 973 ms
2020-05-28 01:55:58.316 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:55:58.620 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:55:58.660 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:55:58.661 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:55:58.661 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602158659
2020-05-28 01:55:58.662 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:55:58.664 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:55:58.667 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:55:58.673 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:55:58.673 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:55:58.673 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602158673
2020-05-28 01:55:58.673 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:55:58.673 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:55:58.675 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:55:58.679 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:55:58.680 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:55:58.680 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602158679
2020-05-28 01:55:58.680 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:55:58.680 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:55:58.681 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:55:58.703 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@3d19d85 via org.mortbay.log.Slf4jLog
2020-05-28 01:55:58.718 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:55:58.735 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.187 seconds (JVM running for 3.362)
2020-05-28 01:55:58.851 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:55:58.851 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:55:58.851 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:55:58.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:55:58.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:55:58.852 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:55:58.855 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:55:58.855 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:55:58.855 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:55:58.879 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:55:58.879 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:55:58.879 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:55:58.879 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:55:58.880 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:55:58.880 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:56:01.890 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 175: {consumer-sensors-1-d819fe2f-1da9-4398-a7f0-7b475b317f9c=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-678896d2-37e2-407e-9b6e-24a33ebb75a8=Assignment(partitions=[]), consumer-sensors-2-2556c7b2-a458-4473-98b6-1b214a28c958=Assignment(partitions=[])}
2020-05-28 01:56:01.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 175
2020-05-28 01:56:01.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 175
2020-05-28 01:56:01.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 175
2020-05-28 01:56:01.899 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:56:01.899 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:56:01.900 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:56:01.900 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:56:01.902 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:56:01.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=113, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:56:01.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:56:01.952 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:56:05.665 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:56:05.665 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:56:06.107 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:56:06.124 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:56:06.124 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:56:07.135 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:56:09.952 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:56:09.952 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:56:09.998 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:56:09.998 [Druid-ConnectionPool-Create-475214267] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:56:09.998 [Druid-ConnectionPool-Create-475214267] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:56:12.983 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:56:12.983 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:56:13.023 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:56:13.024 [Druid-ConnectionPool-Create-475214267] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:56:13.024 [Druid-ConnectionPool-Create-475214267] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:56:33.100 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 01:56:33.102 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:56:33.102 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 01:57:49.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-678896d2-37e2-407e-9b6e-24a33ebb75a8 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:57:49.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 01:57:49.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-2556c7b2-a458-4473-98b6-1b214a28c958 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:57:49.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:57:49.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-d819fe2f-1da9-4398-a7f0-7b475b317f9c sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 01:57:49.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:57:49.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:57:49.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 01:57:49.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:57:49.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:57:49.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 01:57:49.203 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:57:49.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:57:49.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 01:57:49.206 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 01:57:49.287 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 01:57:56.469 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 8268 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 01:57:56.475 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 01:57:57.459 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 01:57:57.467 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 01:57:57.467 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 01:57:57.467 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 01:57:57.580 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 01:57:57.580 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1051 ms
2020-05-28 01:57:57.847 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 01:57:58.168 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:57:58.207 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:57:58.208 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:57:58.208 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602278207
2020-05-28 01:57:58.209 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:57:58.210 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:57:58.214 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:57:58.219 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:57:58.220 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:57:58.220 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602278219
2020-05-28 01:57:58.220 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:57:58.220 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:57:58.221 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 01:57:58.226 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 01:57:58.227 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 01:57:58.227 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590602278226
2020-05-28 01:57:58.227 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 01:57:58.227 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 01:57:58.228 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 01:57:58.249 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@6e3ecf5c via org.mortbay.log.Slf4jLog
2020-05-28 01:57:58.263 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 01:57:58.280 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.22 seconds (JVM running for 3.506)
2020-05-28 01:57:58.390 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:57:58.390 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:57:58.390 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 01:57:58.392 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:57:58.392 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:57:58.392 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 01:57:58.396 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:57:58.396 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:57:58.396 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:57:58.424 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:57:58.424 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:57:58.424 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 01:57:58.425 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 01:57:58.425 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 01:57:58.425 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 01:58:01.438 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 177: {consumer-sensors-1-f49876ed-7587-43c5-a77b-2e5e6585bfab=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-ed6a6759-4043-4d51-8be8-5f9b4ed0c94b=Assignment(partitions=[]), consumer-sensors-3-0499010d-4a1f-4df0-9c25-e16a37b7bcd6=Assignment(partitions=[])}
2020-05-28 01:58:01.448 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 177
2020-05-28 01:58:01.448 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 177
2020-05-28 01:58:01.448 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 177
2020-05-28 01:58:01.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:58:01.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 01:58:01.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:58:01.449 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 01:58:01.450 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 01:58:01.463 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=116, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 01:58:01.463 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 01:58:01.491 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 01:58:06.714 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:58:06.715 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:58:07.151 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:58:07.151 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:58:07.168 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:58:07.169 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:58:08.115 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 01:58:10.479 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:58:10.479 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:58:10.523 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:58:10.523 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:58:10.523 [Druid-ConnectionPool-Create-138560522] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:58:10.524 [Druid-ConnectionPool-Create-138560522] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:58:15.434 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:58:15.435 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:58:15.478 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:58:15.478 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:58:15.478 [Druid-ConnectionPool-Create-138560522] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:58:15.479 [Druid-ConnectionPool-Create-138560522] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:58:21.103 [task-4] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:58:21.104 [task-4] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:58:21.145 [task-4] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:58:21.145 [task-4] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:58:21.145 [Druid-ConnectionPool-Create-138560522] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:58:21.146 [Druid-ConnectionPool-Create-138560522] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 01:58:21.965 [task-5] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 01:58:21.965 [task-5] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 01:58:22.006 [task-5] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 01:58:22.006 [task-5] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 01:58:22.006 [Druid-ConnectionPool-Create-138560522] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 01:58:22.007 [Druid-ConnectionPool-Create-138560522] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 02:03:59.072 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 02:03:59.072 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 02:09:47.627 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into Table cece_data(distinct_id,event,login,properties) values('stff000001c0c480d43abf611d3d','wenWenConsultingExpert',false,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}')]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 02:09:47.628 [task-2] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 02:12:50.186 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Offset commit failed on partition cece_sensors-0 at offset 121: The request timed out.
2020-05-28 02:12:50.186 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 02:12:50.546 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 02:13:58.391 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 02:13:58.391 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 02:14:00.754 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 02:14:18.742 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=1876) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:47:22.002 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:47:22.005 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=1873) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:47:22.051 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:22.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:47:22.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:22.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:47:22.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:22.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:47:22.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:22.771 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:47:23.529 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:23.530 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:47:23.655 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 02:47:23.655 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 02:47:23.657 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.657 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:236) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:463) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1275) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1241) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:23.659 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.659 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.697 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.699 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.702 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.708 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.758 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.762 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1128) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1016) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:547) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:23.785 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.800 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.806 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.810 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.955 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1128) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1016) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:547) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:246) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:463) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1275) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1241) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:23.982 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:23.992 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.001 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.027 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.033 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.284 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.343 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1128) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1016) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:547) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:246) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:463) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1275) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1241) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:24.381 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.407 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.429 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.440 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:24.523 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:24.524 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:47:25.103 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9094 (id: 3 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1128) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1016) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:547) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:161) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:246) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:463) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1275) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1241) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 02:47:25.156 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 02:47:52.014 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:47:55.600 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:48:22.025 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:48:25.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:48:35.454 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 02:48:35.455 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:48:41.760 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 02:48:41.783 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 02:48:42.141 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 02:48:42.141 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:49:05.552 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:49:12.962 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:49:35.554 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:49:42.964 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 02:49:50.783 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 02:49:50.784 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 02:49:57.001 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 02:49:57.028 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 02:49:58.194 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 02:49:58.195 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 03:20:19.004 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 03:20:19.007 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is down
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 03:20:19.007 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 03:20:20.337 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 03:20:20.337 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 03:20:50.158 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 03:20:50.776 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 03:21:20.161 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 03:21:20.778 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 03:21:32.579 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 03:21:32.600 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 03:21:37.452 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 03:21:37.454 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 03:21:38.066 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 03:21:38.067 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 04:15:51.002 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 04:15:52.094 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.BindException: Can't assign requested address
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 04:15:52.094 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 04:15:53.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 04:15:53.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 04:15:54.249 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1308) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1248) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) [kafka-clients-2.5.0.jar:?]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:1089) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1045) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:970) [spring-kafka-2.5.0.RELEASE.jar:2.5.0.RELEASE]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2020-05-28 04:15:54.249 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 04:15:54.402 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 04:15:54.403 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9092 (id: 1 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 04:15:54.403 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Error connecting to node 10.10.87.4:9093 (id: 2 rack: null)
java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:454) ~[?:1.8.0_144]
	at sun.nio.ch.Net.connect(Net.java:446) ~[?:1.8.0_144]
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[?:1.8.0_144]
	at org.apache.kafka.common.network.Selector.doConnect(Selector.java:277) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.common.network.Selector.connect(Selector.java:255) ~[kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:957) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:293) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:495) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:303) [kafka-clients-2.5.0.jar:?]
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1280) [kafka-clients-2.5.0.jar:?]
2020-05-28 04:15:54.422 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 04:16:22.094 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 04:16:25.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:45:00.008 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:45:01.093 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:45:07.113 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:45:07.357 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:45:07.357 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:45:12.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:45:13.321 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:45:13.322 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:45:13.612 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:45:38.299 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:45:44.133 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:46:08.302 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:46:14.135 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:46:23.417 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:46:23.418 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:46:27.800 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:46:28.715 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:46:29.238 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:46:29.239 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:46:54.218 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:47:00.260 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:47:24.221 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:47:30.265 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:47:42.446 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:47:42.447 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:47:46.038 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:47:46.953 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:47:48.505 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:47:48.506 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:48:13.625 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:48:19.536 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:48:43.627 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:48:49.538 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:48:58.950 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:48:58.950 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:49:01.371 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:49:02.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:49:04.858 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:49:04.859 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:49:29.822 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:49:35.967 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:49:59.825 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:50:05.969 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:50:15.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:50:15.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:50:16.595 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:50:17.514 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:50:21.192 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:50:21.192 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:50:46.118 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:50:52.032 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:51:16.121 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:51:22.034 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:51:31.345 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:51:31.345 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:51:31.828 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:51:32.750 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:51:37.266 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:51:37.268 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:52:02.437 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:52:08.422 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:52:32.439 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:52:38.424 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:52:47.052 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:52:47.655 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:52:47.656 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:52:47.972 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:52:53.643 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:52:53.644 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:53:18.522 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:53:24.779 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:53:48.524 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:53:54.780 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:54:02.280 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:54:03.197 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:54:03.745 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:54:03.745 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:54:10.001 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:54:10.004 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:54:34.676 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:54:40.874 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:55:04.678 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:55:10.877 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:55:17.499 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:55:18.418 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:55:19.893 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:55:19.893 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:55:26.089 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:55:26.089 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:55:50.919 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:55:57.280 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:56:20.921 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:56:27.282 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:56:32.730 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:56:33.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:56:36.146 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:56:36.147 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:56:42.507 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:56:42.507 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:57:07.133 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:57:13.435 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:57:37.135 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:57:43.437 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:57:47.963 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 07:57:48.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:57:52.365 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:57:52.366 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:57:58.670 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:57:58.670 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:58:23.469 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:58:29.609 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:58:53.471 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:58:59.611 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:59:03.177 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:59:04.098 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:59:08.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 07:59:08.682 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:59:14.818 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 07:59:14.820 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 07:59:39.772 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 07:59:45.675 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:00:09.773 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:00:15.677 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:00:18.418 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:00:19.337 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:00:25.015 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:00:25.015 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:00:30.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:00:30.916 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:00:55.864 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:01:01.989 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:01:25.866 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:01:31.991 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:01:33.620 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:01:34.538 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:01:41.063 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:01:41.065 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:01:47.197 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:01:47.198 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:02:12.042 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:02:18.199 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:02:42.044 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:02:48.201 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:02:48.859 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:02:49.775 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:02:57.274 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:02:57.275 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:03:03.425 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:03:03.426 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:03:28.131 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:03:34.306 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:03:58.133 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:04:04.060 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:04:04.307 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:04:04.972 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:04:13.324 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:04:13.327 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:04:19.493 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:04:19.495 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:04:44.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:04:50.354 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:05:14.251 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:05:19.274 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:05:20.186 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:05:20.357 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:05:29.472 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:05:29.473 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:05:35.583 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:05:35.584 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:06:00.366 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:06:06.592 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:06:30.368 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:06:34.506 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:06:35.421 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:06:36.593 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:06:45.590 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:06:45.591 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:06:51.814 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:06:51.814 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:07:16.470 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:07:23.008 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:07:46.472 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:07:49.769 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:07:50.684 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:07:53.010 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:08:01.728 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:08:01.731 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:08:08.273 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:08:08.274 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:08:32.732 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:08:39.424 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:09:02.735 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:09:05.049 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:09:05.963 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:09:09.425 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:09:17.998 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:09:17.999 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:09:24.690 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:09:24.690 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:09:49.151 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:09:55.878 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:10:19.152 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:10:20.250 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:10:21.160 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:10:25.879 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:10:34.346 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:10:34.346 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:10:41.076 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:10:41.076 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:11:05.442 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:11:12.129 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:11:35.429 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:11:35.443 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:11:36.338 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:11:42.130 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:11:50.611 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:11:50.611 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:11:57.299 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:11:57.299 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:12:21.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:12:28.110 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:12:50.621 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:12:51.528 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:12:51.695 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:12:58.111 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:13:06.894 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:13:06.894 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:13:13.322 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:13:13.322 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:13:37.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:13:44.227 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:14:05.840 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:14:06.749 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:14:07.709 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:14:14.228 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:14:22.931 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:14:22.932 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:14:29.442 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:14:29.442 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:14:54.017 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:15:00.616 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:15:21.060 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:15:21.968 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:15:24.018 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:15:30.617 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:15:39.230 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:15:39.230 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:15:45.824 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:15:45.824 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:16:10.122 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:16:16.626 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:16:36.247 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:16:37.156 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:16:40.123 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:16:46.627 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:16:55.297 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:16:55.297 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:17:01.802 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:17:01.802 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:17:26.424 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:17:32.834 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:17:51.418 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:17:52.327 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:17:56.425 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:18:02.835 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:18:11.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:18:11.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:18:18.029 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:18:18.029 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:18:42.528 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:18:49.125 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:19:06.617 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:19:07.523 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:19:12.529 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:19:19.126 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:19:27.731 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:19:27.731 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:19:34.315 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:19:34.316 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:19:58.795 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:20:05.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:20:21.803 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:20:22.711 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:20:28.796 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:20:35.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:20:43.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:20:43.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:20:50.435 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:20:50.435 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:21:14.947 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:21:21.297 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:21:36.981 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:21:37.889 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:21:44.948 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:21:51.298 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:22:00.134 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:22:00.134 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:22:06.485 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:22:06.485 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:22:31.311 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:22:37.669 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:22:52.165 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:22:53.078 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:23:01.312 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:23:07.670 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:23:16.488 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:23:16.488 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:23:22.848 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:23:22.848 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:23:47.336 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:23:53.962 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:24:07.346 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:24:08.261 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:24:17.337 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:24:23.963 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:24:32.547 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:24:32.547 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:24:39.177 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:24:39.177 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:25:03.553 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:25:10.140 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:25:22.571 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:25:23.486 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:25:33.554 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:25:40.141 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:25:48.745 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:25:48.745 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:25:55.331 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:25:55.331 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:26:19.691 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:26:26.291 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:26:37.750 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:26:38.665 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:26:49.692 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:26:56.292 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:27:04.883 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:27:04.883 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:27:11.484 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:27:11.484 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:27:35.686 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:27:42.667 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:27:52.954 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 08:27:53.865 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:28:05.687 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:28:12.668 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:28:20.866 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:28:20.866 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:28:27.845 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:28:27.845 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:28:51.672 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:28:59.012 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:29:08.127 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:29:09.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:29:21.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:29:29.013 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:29:36.861 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:29:36.861 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:29:44.214 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:29:44.214 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:30:07.956 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:30:15.394 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:30:23.342 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:30:24.253 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:30:37.957 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:30:45.395 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 08:30:53.331 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 08:30:53.331 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1956200496, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:31:00.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 08:31:00.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1793508076, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 08:31:14.613 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 08:31:14.613 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 08:31:14.613 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 08:31:14.613 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 08:31:14.613 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 08:31:14.613 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 08:31:14.614 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 08:31:14.614 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 08:31:14.619 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 08:31:14.619 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 08:31:14.619 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 08:31:14.621 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 08:31:32.640 [SpringContextShutdownHook] WARN  com.alibaba.druid.pool.DruidDataSource - close connection error
java.sql.SQLException: Error while cleaning up the server resources
	at org.apache.hive.jdbc.HiveConnection.close(HiveConnection.java:839) ~[hive-jdbc-2.3.3.jar:2.3.3]
	at com.alibaba.druid.pool.DruidDataSource.close(DruidDataSource.java:1798) [druid-1.1.9.jar:1.1.9]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.beans.factory.support.DisposableBeanAdapter.invokeCustomDestroyMethod(DisposableBeanAdapter.java:339) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DisposableBeanAdapter.destroy(DisposableBeanAdapter.java:273) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroyBean(DefaultSingletonBeanRegistry.java:579) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingleton(DefaultSingletonBeanRegistry.java:551) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingleton(DefaultListableBeanFactory.java:1091) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingletons(DefaultSingletonBeanRegistry.java:512) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingletons(DefaultListableBeanFactory.java:1084) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.destroyBeans(AbstractApplicationContext.java:1060) [spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:1029) [spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:168) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:948) [spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
Caused by: org.apache.thrift.transport.TTransportException: java.net.SocketException: Operation timed out (Read failed)
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.hive.service.rpc.thrift.TCLIService$Client.recv_CloseSession(TCLIService.java:191) ~[hive-service-rpc-2.3.3.jar:2.3.3]
	at org.apache.hive.service.rpc.thrift.TCLIService$Client.CloseSession(TCLIService.java:178) ~[hive-service-rpc-2.3.3.jar:2.3.3]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:1524) ~[hive-jdbc-2.3.3.jar:2.3.3]
	at com.sun.proxy.$Proxy88.CloseSession(Unknown Source) ~[?:?]
	at org.apache.hive.jdbc.HiveConnection.close(HiveConnection.java:837) ~[hive-jdbc-2.3.3.jar:2.3.3]
	... 16 more
Caused by: java.net.SocketException: Operation timed out (Read failed)
	at java.net.SocketInputStream.socketRead0(Native Method) ~[?:1.8.0_144]
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[?:1.8.0_144]
	at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[?:1.8.0_144]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_144]
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[?:1.8.0_144]
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[?:1.8.0_144]
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_144]
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.hive.service.rpc.thrift.TCLIService$Client.recv_CloseSession(TCLIService.java:191) ~[hive-service-rpc-2.3.3.jar:2.3.3]
	at org.apache.hive.service.rpc.thrift.TCLIService$Client.CloseSession(TCLIService.java:178) ~[hive-service-rpc-2.3.3.jar:2.3.3]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:1524) ~[hive-jdbc-2.3.3.jar:2.3.3]
	at com.sun.proxy.$Proxy88.CloseSession(Unknown Source) ~[?:?]
	at org.apache.hive.jdbc.HiveConnection.close(HiveConnection.java:837) ~[hive-jdbc-2.3.3.jar:2.3.3]
	... 16 more
2020-05-28 08:31:34.230 [SpringContextShutdownHook] WARN  com.alibaba.druid.pool.DruidDataSource - close connection error
java.sql.SQLException: Error while cleaning up the server resources
	at org.apache.hive.jdbc.HiveConnection.close(HiveConnection.java:839) ~[hive-jdbc-2.3.3.jar:2.3.3]
	at com.alibaba.druid.pool.DruidDataSource.close(DruidDataSource.java:1798) [druid-1.1.9.jar:1.1.9]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.springframework.beans.factory.support.DisposableBeanAdapter.invokeCustomDestroyMethod(DisposableBeanAdapter.java:339) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DisposableBeanAdapter.destroy(DisposableBeanAdapter.java:273) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroyBean(DefaultSingletonBeanRegistry.java:579) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingleton(DefaultSingletonBeanRegistry.java:551) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingleton(DefaultListableBeanFactory.java:1091) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingletons(DefaultSingletonBeanRegistry.java:512) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingletons(DefaultListableBeanFactory.java:1084) [spring-beans-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.destroyBeans(AbstractApplicationContext.java:1060) [spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:1029) [spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:168) [spring-boot-2.3.0.RELEASE.jar:2.3.0.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:948) [spring-context-5.2.6.RELEASE.jar:5.2.6.RELEASE]
Caused by: org.apache.thrift.transport.TTransportException: java.net.SocketException: Connection reset
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.hive.service.rpc.thrift.TCLIService$Client.recv_CloseSession(TCLIService.java:191) ~[hive-service-rpc-2.3.3.jar:2.3.3]
	at org.apache.hive.service.rpc.thrift.TCLIService$Client.CloseSession(TCLIService.java:178) ~[hive-service-rpc-2.3.3.jar:2.3.3]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:1524) ~[hive-jdbc-2.3.3.jar:2.3.3]
	at com.sun.proxy.$Proxy88.CloseSession(Unknown Source) ~[?:?]
	at org.apache.hive.jdbc.HiveConnection.close(HiveConnection.java:837) ~[hive-jdbc-2.3.3.jar:2.3.3]
	... 16 more
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:210) ~[?:1.8.0_144]
	at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_144]
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[?:1.8.0_144]
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[?:1.8.0_144]
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_144]
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.hive.service.rpc.thrift.TCLIService$Client.recv_CloseSession(TCLIService.java:191) ~[hive-service-rpc-2.3.3.jar:2.3.3]
	at org.apache.hive.service.rpc.thrift.TCLIService$Client.CloseSession(TCLIService.java:178) ~[hive-service-rpc-2.3.3.jar:2.3.3]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hive.jdbc.HiveConnection$SynchronizedHandler.invoke(HiveConnection.java:1524) ~[hive-jdbc-2.3.3.jar:2.3.3]
	at com.sun.proxy.$Proxy88.CloseSession(Unknown Source) ~[?:?]
	at org.apache.hive.jdbc.HiveConnection.close(HiveConnection.java:837) ~[hive-jdbc-2.3.3.jar:2.3.3]
	... 16 more
2020-05-28 08:31:34.232 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 14:55:24.121 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 24372 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 14:55:24.127 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 14:55:25.254 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 14:55:25.262 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 14:55:25.262 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 14:55:25.262 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 14:55:25.370 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 14:55:25.370 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1209 ms
2020-05-28 14:55:25.659 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 14:55:26.017 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 14:55:26.075 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 14:55:26.076 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 14:55:26.076 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590648926074
2020-05-28 14:55:26.077 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 14:55:26.079 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 14:55:26.084 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 14:55:26.090 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 14:55:26.090 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 14:55:26.090 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590648926090
2020-05-28 14:55:26.090 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 14:55:26.091 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 14:55:26.092 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 14:55:26.097 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 14:55:26.097 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 14:55:26.098 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590648926097
2020-05-28 14:55:26.098 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 14:55:26.098 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 14:55:26.099 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 14:55:26.123 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@2f0ed952 via org.mortbay.log.Slf4jLog
2020-05-28 14:55:26.141 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 14:55:26.158 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.505 seconds (JVM running for 3.474)
2020-05-28 14:55:26.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 14:55:26.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 14:55:26.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 14:55:26.401 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 14:55:26.403 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 14:55:26.403 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 14:55:26.417 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 14:55:26.418 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 14:55:26.419 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 14:55:26.534 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 14:55:26.534 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 14:55:26.534 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 14:55:26.534 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 14:55:27.658 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 14:55:27.658 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 14:55:29.562 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 179: {consumer-sensors-3-652a06ff-c920-4ad1-a95c-3dd16a45e082=Assignment(partitions=[]), consumer-sensors-2-452c0926-7c6b-45b5-83e1-0edd0bb37ec7=Assignment(partitions=[]), consumer-sensors-1-0aae0ed6-1d3b-4397-ba5b-488581994b4e=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 14:55:29.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 179
2020-05-28 14:55:29.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 179
2020-05-28 14:55:29.601 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 179
2020-05-28 14:55:29.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 14:55:29.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 14:55:29.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 14:55:29.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 14:55:29.605 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 14:55:29.634 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=121, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 14:55:29.634 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 14:55:29.694 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 14:55:48.336 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 14:55:48.337 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 14:55:48.806 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 14:55:48.806 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 14:55:48.829 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 14:55:48.829 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 14:55:49.934 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 14:56:19.441 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 14:56:19.441 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 14:58:41.057 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 14:58:41.057 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 14:58:41.172 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 14:58:41.172 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 14:58:42.330 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 14:58:42.330 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 14:58:42.379 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 14:58:42.379 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 14:58:42.379 [Druid-ConnectionPool-Create-389542779] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 14:58:42.379 [Druid-ConnectionPool-Create-389542779] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 14:58:49.720 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 14:58:49.720 [task-2] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:04:37.294 [task-3] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into Table cece_data(distinct_id,event,login,properties) values('stff000001c0c480d43abf611d3d','wenWenConsultingExpert',true,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}')]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 15:04:37.294 [task-3] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:09:54.516 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 15:09:54.516 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-452c0926-7c6b-45b5-83e1-0edd0bb37ec7 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:09:54.516 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-652a06ff-c920-4ad1-a95c-3dd16a45e082 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:09:54.517 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:09:54.517 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-0aae0ed6-1d3b-4397-ba5b-488581994b4e sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:09:54.518 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:09:54.518 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:09:54.518 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:09:54.518 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:09:54.518 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:09:54.518 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:09:54.531 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:09:54.532 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:09:54.533 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:09:54.535 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 15:09:54.647 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 15:09:59.184 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 25032 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 15:09:59.189 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 15:10:00.081 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 15:10:00.089 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 15:10:00.089 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 15:10:00.090 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 15:10:00.191 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 15:10:00.192 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 969 ms
2020-05-28 15:10:00.443 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 15:10:00.735 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:10:00.775 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:10:00.776 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:10:00.776 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590649800775
2020-05-28 15:10:00.777 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:10:00.778 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:10:00.781 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:10:00.787 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:10:00.787 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:10:00.787 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590649800787
2020-05-28 15:10:00.788 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:10:00.788 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:10:00.789 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:10:00.794 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:10:00.794 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:10:00.794 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590649800794
2020-05-28 15:10:00.795 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:10:00.795 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:10:00.796 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 15:10:00.815 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@8cc8cdb via org.mortbay.log.Slf4jLog
2020-05-28 15:10:00.828 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 15:10:00.846 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 1.982 seconds (JVM running for 2.745)
2020-05-28 15:10:01.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:10:01.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:10:01.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:10:01.094 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:10:01.094 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:10:01.094 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:10:01.096 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:10:01.096 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:10:01.096 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:10:01.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:10:01.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:10:01.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:10:01.155 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:10:01.155 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:10:01.155 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:10:04.168 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 181: {consumer-sensors-1-657c6faa-3744-43b0-9bbd-fcda269e6e89=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-8ed5da77-632e-4d7c-8043-a2053b62d6af=Assignment(partitions=[]), consumer-sensors-2-5e107956-0a9b-43db-8a7c-7863c5bbd01c=Assignment(partitions=[])}
2020-05-28 15:10:04.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 181
2020-05-28 15:10:04.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 15:10:04.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 181
2020-05-28 15:10:04.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 181
2020-05-28 15:10:04.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:10:04.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:10:04.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:10:04.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:10:04.204 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=124, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 15:10:04.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 15:10:04.262 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:10:19.119 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:10:19.120 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:10:19.794 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:10:19.795 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:10:19.811 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:10:19.812 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:10:20.918 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 15:10:21.009 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; bad SQL grammar [insert into cece_data select 'stff000001c0c480d43abf611d3d' distinct_id,'wenWenConsultingExpert' event,true login,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}' properties;]; nested exception is org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:246 extraneous input ';' expecting EOF near '<EOF>'
2020-05-28 15:10:21.010 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:10:39.644 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 15:10:39.644 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-5e107956-0a9b-43db-8a7c-7863c5bbd01c sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:10:39.644 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-8ed5da77-632e-4d7c-8043-a2053b62d6af sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:10:39.644 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:10:39.644 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-657c6faa-3744-43b0-9bbd-fcda269e6e89 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:10:39.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:10:39.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:10:39.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:10:39.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:10:39.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:10:39.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:10:39.666 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:10:39.666 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:10:39.666 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:10:39.668 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 15:10:39.702 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 15:10:44.474 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 25077 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 15:10:44.480 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 15:10:45.337 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 15:10:45.344 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 15:10:45.344 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 15:10:45.344 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 15:10:45.425 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 15:10:45.426 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 906 ms
2020-05-28 15:10:45.716 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 15:10:46.015 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:10:46.054 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:10:46.054 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:10:46.055 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590649846053
2020-05-28 15:10:46.056 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:10:46.057 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:10:46.060 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:10:46.066 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:10:46.066 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:10:46.066 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590649846066
2020-05-28 15:10:46.067 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:10:46.067 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:10:46.068 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:10:46.073 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:10:46.073 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:10:46.073 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590649846073
2020-05-28 15:10:46.073 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:10:46.074 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:10:46.074 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 15:10:46.094 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@2121d1f9 via org.mortbay.log.Slf4jLog
2020-05-28 15:10:46.107 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 15:10:46.123 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.086 seconds (JVM running for 2.85)
2020-05-28 15:10:46.278 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:10:46.278 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:10:46.278 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:10:46.279 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:10:46.279 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:10:46.279 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:10:46.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:10:46.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:10:46.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:10:46.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:10:46.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:10:46.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:10:46.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:10:46.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:10:46.319 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:10:49.351 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 183: {consumer-sensors-3-0863b85a-d1ed-4c8e-8db0-4c79aa460bb3=Assignment(partitions=[]), consumer-sensors-2-d5b4fa10-ead4-48ca-80ac-9e3aebbfe1f4=Assignment(partitions=[]), consumer-sensors-1-bee54411-1357-4cd9-bd57-0403f1643438=Assignment(partitions=[cece_sensors-0, oversea_sensors-0])}
2020-05-28 15:10:49.363 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 183
2020-05-28 15:10:49.363 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 183
2020-05-28 15:10:49.363 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 183
2020-05-28 15:10:49.363 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:10:49.363 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:10:49.363 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:10:49.363 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:10:49.365 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 15:10:49.384 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=125, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 15:10:49.384 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 15:10:49.425 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:10:53.210 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:10:53.211 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:10:53.609 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:10:53.609 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:10:53.625 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:10:53.625 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:10:54.858 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 15:10:55.223 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; bad SQL grammar [insert into cece_data select 'stff000001c0c480d43abf611d3d' distinct_id,'wenWenConsultingExpert' event,true login,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}' properties]; nested exception is org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10044]: Line 1:12 Cannot insert into target table because column number/types are different 'cece_data': Table insclause-0 has 5 columns, but query has 4 columns.
2020-05-28 15:10:55.223 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:13:53.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-0863b85a-d1ed-4c8e-8db0-4c79aa460bb3 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:13:53.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-d5b4fa10-ead4-48ca-80ac-9e3aebbfe1f4 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:13:53.270 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 15:13:53.272 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:13:53.272 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-bee54411-1357-4cd9-bd57-0403f1643438 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:13:53.273 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:13:53.273 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:13:53.273 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:13:53.273 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:13:53.273 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:13:53.273 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:13:53.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:13:53.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:13:53.291 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:13:53.292 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 15:13:53.323 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 15:13:58.007 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 25235 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 15:13:58.013 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 15:13:59.091 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 15:13:59.098 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 15:13:59.099 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 15:13:59.099 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 15:13:59.184 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 15:13:59.184 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 1126 ms
2020-05-28 15:13:59.478 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 15:13:59.768 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:13:59.809 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:13:59.809 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:13:59.809 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650039808
2020-05-28 15:13:59.811 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:13:59.812 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:13:59.816 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:13:59.821 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:13:59.822 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:13:59.822 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650039821
2020-05-28 15:13:59.822 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:13:59.822 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:13:59.823 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:13:59.828 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:13:59.829 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:13:59.829 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650039828
2020-05-28 15:13:59.829 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:13:59.829 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:13:59.830 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 15:13:59.849 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@37496720 via org.mortbay.log.Slf4jLog
2020-05-28 15:13:59.862 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 15:13:59.878 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.318 seconds (JVM running for 3.186)
2020-05-28 15:13:59.999 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:13:59.999 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:13:59.999 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:14:00.000 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:14:00.000 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:14:00.000 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:14:00.003 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:14:00.003 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:14:00.003 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:14:00.035 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:14:00.036 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:14:00.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:14:00.153 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:14:00.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:14:00.154 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:14:03.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 185: {consumer-sensors-1-344d3831-d66b-4451-a7f2-56d37d07776f=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-ee982dcd-517e-4a79-bfb2-5f5acd5b8b43=Assignment(partitions=[]), consumer-sensors-2-9d37d48f-17e5-473b-940a-b66bddf22be8=Assignment(partitions=[])}
2020-05-28 15:14:03.081 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 185
2020-05-28 15:14:03.081 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 185
2020-05-28 15:14:03.081 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 185
2020-05-28 15:14:03.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:14:03.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:14:03.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:14:03.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:14:03.084 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 15:14:03.097 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=126, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 15:14:03.098 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 15:14:03.141 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:14:15.625 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:14:15.625 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:14:16.025 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:14:16.026 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:14:16.042 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:14:16.042 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:14:17.097 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 15:14:17.778 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:14:17.778 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:14:17.854 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:14:17.854 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:14:17.854 [Druid-ConnectionPool-Create-1505624785] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:14:17.854 [Druid-ConnectionPool-Create-1505624785] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:14:54.038 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 15:14:54.038 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:18:14.391 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Offset commit failed on partition cece_sensors-0 at offset 128: The request timed out.
2020-05-28 15:18:14.393 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 15:18:14.608 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:19:41.426 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Offset commit failed on partition cece_sensors-0 at offset 128: The request timed out.
2020-05-28 15:19:41.427 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 15:19:41.797 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:20:38.807 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 15:20:38.807 [task-2] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:21:32.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-ee982dcd-517e-4a79-bfb2-5f5acd5b8b43 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:21:32.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 15:21:32.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-9d37d48f-17e5-473b-940a-b66bddf22be8 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:21:32.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:21:32.289 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-344d3831-d66b-4451-a7f2-56d37d07776f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:21:32.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:21:32.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:21:32.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:21:32.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:21:32.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:21:32.290 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:21:32.307 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:21:32.308 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:21:32.309 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:21:32.311 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 15:21:32.424 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 15:21:34.710 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 25563 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 15:21:34.714 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 15:21:35.565 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 15:21:35.572 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 15:21:35.572 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 15:21:35.572 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 15:21:35.660 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 15:21:35.660 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 916 ms
2020-05-28 15:21:35.918 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 15:21:36.215 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:21:36.254 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:21:36.255 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:21:36.255 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650496254
2020-05-28 15:21:36.256 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:21:36.257 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:21:36.261 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:21:36.266 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:21:36.266 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:21:36.266 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650496266
2020-05-28 15:21:36.267 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:21:36.267 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:21:36.268 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:21:36.273 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:21:36.273 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:21:36.273 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650496273
2020-05-28 15:21:36.274 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:21:36.274 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:21:36.275 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 15:21:36.295 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@e146f93 via org.mortbay.log.Slf4jLog
2020-05-28 15:21:36.309 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 15:21:36.326 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.014 seconds (JVM running for 2.98)
2020-05-28 15:21:36.438 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:21:36.438 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:21:36.438 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:21:36.439 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:21:36.439 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:21:36.439 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:21:36.441 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:21:36.441 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:21:36.441 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:21:36.475 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:21:36.475 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:21:36.475 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:21:36.475 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:21:36.476 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:21:36.476 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:21:39.516 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 187: {consumer-sensors-1-8d03cafb-1fa1-4872-bd52-97f07b3a9d9e=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-c22f0f86-b8b0-4cfc-a565-f41393ea7c53=Assignment(partitions=[]), consumer-sensors-2-c3b281ba-76e9-45a1-a0b9-fcceab39aeb6=Assignment(partitions=[])}
2020-05-28 15:21:39.530 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 187
2020-05-28 15:21:39.530 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 187
2020-05-28 15:21:39.530 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 187
2020-05-28 15:21:39.531 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:21:39.531 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:21:39.531 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:21:39.531 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:21:39.533 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 15:21:39.556 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=128, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 15:21:39.556 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 15:21:39.602 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:21:47.558 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:21:47.558 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:21:48.011 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:21:48.011 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:21:48.027 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:21:48.028 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:21:49.232 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 15:22:19.723 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into cece_data select row_number() over() as id,'stff000001c0c480d43abf611d3d' distinct_id,'wenWenConsultingExpert' event,true login,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}' properties]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 15:22:19.723 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:22:27.988 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:22:27.989 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:22:28.096 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:22:28.096 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:22:53.945 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into cece_data select row_number() over() as id,'stff000001c0c480d43abf611d3d' distinct_id,'wenWenConsultingExpert' event,true login,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}' properties]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1587724197076_0075_2_00, diagnostics=[Task failed, taskId=task_1587724197076_0075_2_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Container container_1587724197076_0075_01_000005 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000005
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 1 failed, info=[Container container_1587724197076_0075_01_000007 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000007
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 2 failed, info=[Container container_1587724197076_0075_01_000009 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000009
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 3 failed, info=[Container container_1587724197076_0075_01_000011 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000011
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1587724197076_0075_2_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1587724197076_0075_2_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1587724197076_0075_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1587724197076_0075_2_00, diagnostics=[Task failed, taskId=task_1587724197076_0075_2_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Container container_1587724197076_0075_01_000005 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000005
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 1 failed, info=[Container container_1587724197076_0075_01_000007 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000007
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 2 failed, info=[Container container_1587724197076_0075_01_000009 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000009
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 3 failed, info=[Container container_1587724197076_0075_01_000011 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000011
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1587724197076_0075_2_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1587724197076_0075_2_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1587724197076_0075_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:196)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1587724197076_0075_2_00, diagnostics=[Task failed, taskId=task_1587724197076_0075_2_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Container container_1587724197076_0075_01_000005 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000005
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 1 failed, info=[Container container_1587724197076_0075_01_000007 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000007
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 2 failed, info=[Container container_1587724197076_0075_01_000009 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000009
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 3 failed, info=[Container container_1587724197076_0075_01_000011 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000011
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1587724197076_0075_2_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1587724197076_0075_2_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1587724197076_0075_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1587724197076_0075_2_00, diagnostics=[Task failed, taskId=task_1587724197076_0075_2_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Container container_1587724197076_0075_01_000005 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000005
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 1 failed, info=[Container container_1587724197076_0075_01_000007 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000007
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 2 failed, info=[Container container_1587724197076_0075_01_000009 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000009
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]], TaskAttempt 3 failed, info=[Container container_1587724197076_0075_01_000011 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.
Container id: container_1587724197076_0075_01_000011
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:604)
	at org.apache.hadoop.util.Shell.run(Shell.java:507)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:789)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Container exited with a non-zero exit code 1
]]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1587724197076_0075_2_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1587724197076_0075_2_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1587724197076_0075_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:196)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more

2020-05-28 15:22:53.946 [task-2] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:23:37.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-c22f0f86-b8b0-4cfc-a565-f41393ea7c53 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:23:37.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-c3b281ba-76e9-45a1-a0b9-fcceab39aeb6 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:23:37.080 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 15:23:37.081 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:23:37.081 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-8d03cafb-1fa1-4872-bd52-97f07b3a9d9e sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:23:37.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:23:37.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:23:37.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:23:37.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:23:37.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:23:37.082 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:23:37.097 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:23:37.097 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:23:37.098 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:23:37.099 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 15:23:37.174 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 15:23:40.038 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 25659 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 15:23:40.043 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 15:23:40.869 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 15:23:40.876 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 15:23:40.877 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 15:23:40.877 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 15:23:40.981 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 15:23:40.981 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 902 ms
2020-05-28 15:23:41.230 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 15:23:41.529 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:23:41.568 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:23:41.568 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:23:41.568 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650621567
2020-05-28 15:23:41.570 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:23:41.571 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:23:41.574 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:23:41.580 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:23:41.580 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:23:41.580 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650621580
2020-05-28 15:23:41.580 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:23:41.581 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:23:41.582 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:23:41.587 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:23:41.587 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:23:41.587 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650621587
2020-05-28 15:23:41.587 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:23:41.587 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:23:41.588 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 15:23:41.608 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@44af588b via org.mortbay.log.Slf4jLog
2020-05-28 15:23:41.620 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 15:23:41.638 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 1.958 seconds (JVM running for 2.806)
2020-05-28 15:23:41.757 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:23:41.757 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:23:41.757 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:23:41.759 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:23:41.759 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:23:41.759 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:23:41.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:23:41.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:23:41.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:23:41.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:23:41.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:23:41.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:23:41.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:23:41.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:23:41.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:23:44.799 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Finished assignment for group at generation 189: {consumer-sensors-3-1a799686-48a5-43e5-af9b-7d61141339b3=Assignment(partitions=[]), consumer-sensors-1-d300413e-da97-4641-b9db-79f8951938ed=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-2928fc8c-0768-4101-9cac-b9a38060897f=Assignment(partitions=[])}
2020-05-28 15:23:44.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 189
2020-05-28 15:23:44.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 189
2020-05-28 15:23:44.811 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 189
2020-05-28 15:23:44.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:23:44.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:23:44.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:23:44.812 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:23:44.814 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 15:23:44.826 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=130, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 15:23:44.826 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 15:23:44.861 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:23:45.766 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:23:45.766 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:23:46.142 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:23:46.142 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:23:46.158 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:23:46.158 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:23:47.202 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 15:24:28.453 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into cece_data select row_number() over() as id,'stff000001c0c480d43abf611d3d' distinct_id,'wenWenConsultingExpert' event,true login,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}' properties]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 15:24:28.453 [task-1] INFO  com.queue.process.kafka.controller.SensorsController - end;
2020-05-28 15:25:36.816 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-1a799686-48a5-43e5-af9b-7d61141339b3 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:25:36.816 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-2928fc8c-0768-4101-9cac-b9a38060897f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:25:36.816 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 15:25:36.817 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:25:36.818 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-d300413e-da97-4641-b9db-79f8951938ed sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:25:36.818 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:25:36.818 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:25:36.818 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:25:36.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:25:36.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:25:36.819 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:25:36.837 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:25:36.837 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:25:36.837 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:25:36.839 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 15:25:36.963 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 15:25:41.319 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 25751 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 15:25:41.324 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 15:25:42.220 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 15:25:42.228 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 15:25:42.228 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 15:25:42.228 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 15:25:42.336 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 15:25:42.336 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 969 ms
2020-05-28 15:25:42.588 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 15:25:42.890 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:25:42.930 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:25:42.930 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:25:42.930 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650742929
2020-05-28 15:25:42.932 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:25:42.933 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:25:42.936 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:25:42.943 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:25:42.943 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:25:42.943 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650742943
2020-05-28 15:25:42.943 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:25:42.943 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:25:42.945 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:25:42.950 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:25:42.950 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:25:42.950 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650742950
2020-05-28 15:25:42.951 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:25:42.951 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:25:42.952 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 15:25:42.972 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@4832f03b via org.mortbay.log.Slf4jLog
2020-05-28 15:25:42.986 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 15:25:43.004 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.066 seconds (JVM running for 2.867)
2020-05-28 15:25:43.128 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:25:43.128 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:25:43.128 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:25:43.129 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:25:43.129 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:25:43.129 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:25:43.131 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:25:43.131 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:25:43.131 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:25:43.158 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:25:43.158 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:25:43.167 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:25:43.167 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:25:43.167 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:25:43.167 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:25:46.205 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 191: {consumer-sensors-1-559acb66-0e3b-4974-840f-5c744aa3f8e5=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-10fb9b9e-f818-40be-a390-faa362fbc693=Assignment(partitions=[]), consumer-sensors-2-e5cac2f2-65d2-43d4-915a-ea9d8f897e6f=Assignment(partitions=[])}
2020-05-28 15:25:46.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 191
2020-05-28 15:25:46.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 191
2020-05-28 15:25:46.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 191
2020-05-28 15:25:46.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:25:46.219 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:25:46.220 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:25:46.220 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:25:46.221 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 15:25:46.234 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=131, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 15:25:46.235 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 15:25:46.280 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:25:53.402 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:25:53.403 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:25:53.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:25:53.761 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:25:53.778 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:25:53.778 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:25:55.015 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 15:26:26.087 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into cece_data select row_number() over() as id,'stff000001c0c480d43abf611d3d' distinct_id,'wenWenConsultingExpert' event,true login,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}' properties]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 15:26:26.098 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:26:26.098 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:26:26.165 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:26:26.166 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:26:29.620 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Error: StatementCallback; SQL [insert into cece_data select row_number() over() as id,'stff000001c0c480d43abf611d3d' distinct_id,'wenWenConsultingExpert' event,true login,'{"problem_description":"测试","is_use_question_coupon":false,"bount":20,"consult_type":"stars","hoping_talents_number":0}' properties]; org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more
; nested exception is java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. User admin does not have privileges for ALTERTABLE_ADDCOLS
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:625)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:605)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2054)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:360)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
	... 11 more
Caused by: MetaException(message:User admin does not have privileges for ALTERTABLE_ADDCOLS)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59598)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:375)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:322)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
	at com.sun.proxy.$Proxy35.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:623)
	... 22 more

2020-05-28 15:26:33.267 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 15:26:33.267 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-10fb9b9e-f818-40be-a390-faa362fbc693 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:26:33.267 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-e5cac2f2-65d2-43d4-915a-ea9d8f897e6f sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:26:33.268 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:26:33.268 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-559acb66-0e3b-4974-840f-5c744aa3f8e5 sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:26:33.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:26:33.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:26:33.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:26:33.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:26:33.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:26:33.269 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:26:33.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:26:33.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:26:33.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:26:33.289 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 15:26:33.343 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 15:27:07.096 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 25822 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 15:27:07.103 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 15:27:08.018 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 15:27:08.025 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 15:27:08.026 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 15:27:08.026 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 15:27:08.109 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 15:27:08.109 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 961 ms
2020-05-28 15:27:08.368 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 15:27:08.672 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:27:08.712 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:27:08.712 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:27:08.712 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650828711
2020-05-28 15:27:08.714 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:27:08.715 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:27:08.718 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:27:08.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:27:08.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:27:08.724 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650828724
2020-05-28 15:27:08.725 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:27:08.725 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:27:08.726 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:27:08.731 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:27:08.731 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:27:08.731 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590650828731
2020-05-28 15:27:08.732 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:27:08.732 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:27:08.733 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 15:27:08.754 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@7301eebe via org.mortbay.log.Slf4jLog
2020-05-28 15:27:08.767 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 15:27:08.784 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.099 seconds (JVM running for 3.001)
2020-05-28 15:27:08.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:27:08.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:27:08.898 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:27:08.900 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:27:08.900 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:27:08.900 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:27:08.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:27:08.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:27:08.901 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:27:08.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:27:08.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:27:08.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:27:08.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:27:08.929 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:27:08.929 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:27:11.944 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Finished assignment for group at generation 193: {consumer-sensors-1-ddc7ddd2-c717-4543-bd72-6e5dbfbb9b4e=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-2-b10f122e-9d8a-4aba-83a2-7b3a484db97e=Assignment(partitions=[]), consumer-sensors-3-a2b75dc1-80d8-4deb-9893-ba9b56870a1e=Assignment(partitions=[])}
2020-05-28 15:27:11.958 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 193
2020-05-28 15:27:11.958 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 193
2020-05-28 15:27:11.958 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 193
2020-05-28 15:27:11.959 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:27:11.959 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:27:11.959 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:27:11.959 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:27:11.960 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 15:27:11.975 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=133, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 15:27:11.975 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 15:27:12.011 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:27:17.676 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:27:17.677 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:27:18.080 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:27:18.081 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:27:18.097 [task-1] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:27:18.098 [task-1] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:27:19.179 [task-1] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} inited
2020-05-28 15:27:21.255 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors Start: 
2020-05-28 15:27:21.255 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors target: https://log.cece.com/sa?project=default
2020-05-28 15:27:21.333 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Report Sensors End; 
2020-05-28 15:27:21.334 [task-2] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive Start: 
2020-05-28 15:27:21.334 [Druid-ConnectionPool-Create-1254995589] INFO  org.apache.hive.jdbc.Utils - Supplied authorities: 10.10.81.216:10000
2020-05-28 15:27:21.334 [Druid-ConnectionPool-Create-1254995589] INFO  org.apache.hive.jdbc.Utils - Resolved authority: 10.10.81.216:10000
2020-05-28 15:27:55.409 [task-1] INFO  com.queue.process.kafka.service.impl.SensorsServiceImpl - Synchronize Report Data To Hive End; 
2020-05-28 15:30:59.185 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Offset commit failed on partition cece_sensors-0 at offset 135: The request timed out.
2020-05-28 15:30:59.185 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 15:30:59.378 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:31:11.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Member consumer-sensors-2-b10f122e-9d8a-4aba-83a2-7b3a484db97e sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:31:11.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Member consumer-sensors-3-a2b75dc1-80d8-4deb-9893-ba9b56870a1e sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:31:11.706 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Revoke previously assigned partitions cece_sensors-0, oversea_sensors-0
2020-05-28 15:31:11.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions revoked: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:31:11.707 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Member consumer-sensors-1-ddc7ddd2-c717-4543-bd72-6e5dbfbb9b4e sending LeaveGroup request to coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
2020-05-28 15:31:11.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:31:11.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:31:11.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Unsubscribed all topics or patterns and assigned partitions
2020-05-28 15:31:11.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:31:11.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:31:11.708 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Shutting down ExecutorService
2020-05-28 15:31:11.730 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:31:11.730 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:31:11.730 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - sensors: Consumer stopped
2020-05-28 15:31:11.733 [SpringContextShutdownHook] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Shutting down ExecutorService 'applicationTaskExecutor'
2020-05-28 15:31:11.804 [SpringContextShutdownHook] INFO  com.alibaba.druid.pool.DruidDataSource - {dataSource-1} closed
2020-05-28 15:31:14.967 [main] INFO  com.queue.process.QueueApplication - Starting QueueApplication on lizhe.local with PID 27563 (/Users/lizhe/docker/app/work/java/SpringBoot/queueprocess/target/classes started by lizhe in /Users/lizhe/docker/app/work/java/SpringBoot/queueprocess)
2020-05-28 15:31:14.971 [main] INFO  com.queue.process.QueueApplication - No active profile set, falling back to default profiles: default
2020-05-28 15:31:15.875 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 8088 (http)
2020-05-28 15:31:15.882 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8088"]
2020-05-28 15:31:15.883 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2020-05-28 15:31:15.883 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.35]
2020-05-28 15:31:15.983 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2020-05-28 15:31:15.983 [main] INFO  org.springframework.web.context.ContextLoader - Root WebApplicationContext: initialization completed in 971 ms
2020-05-28 15:31:16.272 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - Initializing ExecutorService 'applicationTaskExecutor'
2020-05-28 15:31:16.591 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:31:16.632 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:31:16.633 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:31:16.633 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590651076631
2020-05-28 15:31:16.635 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-1, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:31:16.636 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:31:16.640 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:31:16.656 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:31:16.657 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:31:16.657 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590651076656
2020-05-28 15:31:16.658 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-2, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:31:16.658 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:31:16.660 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.10.87.4:9092, 10.10.87.4:9093, 10.10.87.4:9094]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sensors
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-05-28 15:31:16.670 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.5.0
2020-05-28 15:31:16.670 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 66563e712b0b9f84
2020-05-28 15:31:16.670 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1590651076670
2020-05-28 15:31:16.671 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-sensors-3, groupId=sensors] Subscribed to topic(s): cece_sensors, oversea_sensors
2020-05-28 15:31:16.671 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService
2020-05-28 15:31:16.672 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8088"]
2020-05-28 15:31:16.698 [main] INFO  org.mortbay.log - Logging to org.apache.logging.slf4j.Log4jLogger@32e652b6 via org.mortbay.log.Slf4jLog
2020-05-28 15:31:16.713 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 8088 (http) with context path ''
2020-05-28 15:31:16.733 [main] INFO  com.queue.process.QueueApplication - Started QueueApplication in 2.276 seconds (JVM running for 3.183)
2020-05-28 15:31:16.882 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-1, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:31:16.882 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-2, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:31:16.882 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-sensors-3, groupId=sensors] Cluster ID: sFHdtbuQQUqJYljSPsbULg
2020-05-28 15:31:16.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:31:16.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:31:16.883 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Discovered group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null)
2020-05-28 15:31:16.896 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:31:16.896 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:31:16.896 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:31:16.944 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:31:16.944 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] (Re-)joining group
2020-05-28 15:31:16.948 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:31:16.948 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] (Re-)joining group
2020-05-28 15:31:16.949 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-05-28 15:31:16.949 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] (Re-)joining group
2020-05-28 15:31:19.957 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Finished assignment for group at generation 195: {consumer-sensors-1-24cf9730-3eaa-44ed-8897-2ccb90ff5c70=Assignment(partitions=[cece_sensors-0, oversea_sensors-0]), consumer-sensors-3-b8234f3e-b9aa-451d-b807-fd02fca29bf8=Assignment(partitions=[]), consumer-sensors-2-89694520-9b55-4f5c-9a93-a7fd6e4203a8=Assignment(partitions=[])}
2020-05-28 15:31:19.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Successfully joined group with generation 195
2020-05-28 15:31:19.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:31:19.968 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:31:19.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Successfully joined group with generation 195
2020-05-28 15:31:19.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Successfully joined group with generation 195
2020-05-28 15:31:19.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Adding newly assigned partitions: 
2020-05-28 15:31:19.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: []
2020-05-28 15:31:19.971 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Adding newly assigned partitions: cece_sensors-0, oversea_sensors-0
2020-05-28 15:31:19.986 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition cece_sensors-0 to the committed offset FetchPosition{offset=135, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9093 (id: 2 rack: null)], epoch=0}}
2020-05-28 15:31:19.986 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Setting offset for partition oversea_sensors-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[10.10.87.4:9092 (id: 1 rack: null)], epoch=0}}
2020-05-28 15:31:20.031 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.springframework.kafka.listener.KafkaMessageListenerContainer - sensors: partitions assigned: [cece_sensors-0, oversea_sensors-0]
2020-05-28 15:33:36.128 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-1, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 15:33:36.203 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-3, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 15:33:36.203 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-sensors-2, groupId=sensors] Group coordinator 10.10.87.4:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2020-05-28 15:33:54.393 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=246) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:33:58.077 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=247) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:34:24.437 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:34:28.121 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:34:51.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:34:51.989 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:34:54.438 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:34:58.122 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:35:10.144 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:35:10.145 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:35:13.827 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:35:13.828 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:35:40.233 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:35:43.908 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:36:07.646 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:36:07.646 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:36:10.234 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:36:13.910 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:36:25.778 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:36:25.778 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:36:29.434 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:36:29.434 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:36:55.983 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:36:59.657 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:37:23.099 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:37:23.100 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:37:25.985 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:37:29.658 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:37:41.446 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:37:41.447 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:37:45.127 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:37:45.127 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:38:11.916 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:38:15.507 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:38:38.620 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:38:38.620 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:38:41.917 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:38:45.508 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:38:57.454 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:38:57.454 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:39:01.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:39:01.045 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:39:28.399 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:39:31.712 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:39:54.122 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:39:54.123 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:39:58.400 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:40:01.713 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:40:13.942 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:40:13.942 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:40:17.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:40:17.264 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:40:44.744 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:40:48.147 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:41:09.718 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:41:09.718 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:41:14.745 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:41:18.148 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:41:30.329 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:41:30.329 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:41:33.744 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:41:33.744 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:42:01.435 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:42:04.631 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:42:25.405 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:42:25.405 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:42:31.436 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:42:34.632 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:42:47.097 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:42:47.097 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:42:50.283 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:42:50.283 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:43:17.926 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:43:21.402 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:43:40.960 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:43:40.960 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:43:47.927 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:43:51.403 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:44:03.548 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:44:03.549 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:44:07.029 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:44:07.029 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:44:34.406 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:44:37.972 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:44:56.587 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:44:56.587 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:45:04.407 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:45:07.973 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:45:20.093 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:45:20.094 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:45:23.677 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:45:23.677 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:45:50.965 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:45:54.869 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:46:13.600 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:46:13.600 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:46:20.967 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:46:24.870 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:46:37.874 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:46:37.874 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:46:41.758 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:46:41.758 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:47:09.036 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:47:12.763 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:47:29.199 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:47:29.199 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:47:39.037 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:47:42.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:47:54.551 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:47:54.551 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:47:58.274 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:47:58.274 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:48:25.426 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:48:29.447 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:48:44.662 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:48:44.662 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:48:55.427 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:48:59.448 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:49:11.052 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:49:11.052 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:49:15.081 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:49:15.081 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:49:42.112 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:49:46.237 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:50:00.351 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:50:00.351 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:50:12.113 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:50:16.238 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:50:27.715 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:50:27.715 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:50:31.842 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:50:31.842 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:50:58.673 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:51:02.712 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:51:15.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:51:15.928 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:51:28.675 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:51:32.713 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:51:44.168 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:51:44.169 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:51:48.194 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:51:48.195 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:52:15.004 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:52:19.183 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:52:31.398 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:52:31.398 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:52:45.006 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:52:49.184 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:53:00.474 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:53:00.475 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:53:04.662 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:53:04.663 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:53:31.314 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:53:35.772 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:53:46.933 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:53:46.933 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:54:01.315 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:54:05.773 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:54:16.860 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:54:16.860 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:54:21.310 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:54:21.311 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:54:48.002 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:54:52.125 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:55:02.516 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:55:02.516 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:55:18.003 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:55:22.126 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:55:33.609 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:55:33.609 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:55:37.728 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:55:37.728 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:56:04.506 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:56:08.543 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:56:17.979 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:56:17.979 [org.springframework.kafka.KafkaListenerEndpointContainer#0-1-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:56:34.507 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:56:38.544 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:56:49.912 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:56:49.912 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:56:53.948 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:56:53.948 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:57:20.816 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:57:24.787 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:57:33.378 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-2, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:57:33.378 [org.springframework.kafka.KafkaListenerEndpointContainer#0-2-C-1] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-3, groupId=sensors] Connection to node 3 (/10.10.87.4:9094) could not be established. Broker may not be available.
2020-05-28 15:57:50.817 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:57:54.788 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.
2020-05-28 15:58:06.204 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 1 (/10.10.87.4:9092) could not be established. Broker may not be available.
2020-05-28 15:58:06.204 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=704485618, epoch=INITIAL) to node 1: {}.
org.apache.kafka.common.errors.DisconnectException: null
2020-05-28 15:58:10.174 [kafka-coordinator-heartbeat-thread | sensors] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-sensors-1, groupId=sensors] Connection to node 2 (/10.10.87.4:9093) could not be established. Broker may not be available.
2020-05-28 15:58:10.174 [kafka-coordinator-heartbeat-thread | sensors] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-sensors-1, groupId=sensors] Error sending fetch request (sessionId=1665825717, epoch=INITIAL) to node 2: {}.
org.apache.kafka.common.errors.DisconnectException: null
